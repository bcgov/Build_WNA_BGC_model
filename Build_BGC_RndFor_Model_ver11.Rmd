---
title: "Random Forest Model of Biogeoclimatic Units for Western North America"
author: "William H MacKenzie & Kiri Daust"
date: "22/03/2020"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
require (smotefamily)
require(data.table)
library(randtoolbox)
library(clhs)
library(foreign)
library(ggplot2)
library(raster)
library(RStoolbox)
library(maptools)
library(sp)
library(spatstat)
require(doParallel)
require(scales)
require(sf)
require(caret)
require (gstat)
require(ranger)
require(rticles)
require(expss)
require(tictoc)
require(randomForest)
library(tidyverse)
require(tidymodels)
require(vip)
require(stringr)
require(corpcor)
#require (gstat)
#install.packages("smotefamily", dependencies = TRUE)
#data.dir = "./inputs/"
#dir.create("cLHS")

addVars <- function(dat){
  dat$PPT_MJ <- dat$PPT05 + dat$PPT06  # MaY/June precip
  dat$PPT_JAS <- dat$PPT07 + dat$PPT08 + dat$PPT09  # July/Aug/Sept precip
  dat$PPT.dormant <- dat$PPT_at + dat$PPT_wt  # for calculating spring deficit
  dat$CMD.def <- 500 - (dat$PPT.dormant)  # start of growing season deficit original value was 400 but 500 seems better
  dat$CMD.def[dat$CMD.def < 0] <- 0  #negative values set to zero = no deficit
  dat$CMDMax <- dat$CMD07
  dat$CMD.total <- dat$CMD.def + dat$CMD
  dat$CMD.grow <- dat$CMD05 + dat$CMD06 +dat$CMD07 +dat$CMD08 +dat$CMD09
  dat$DD5.grow <- dat$DD5_05 + dat$DD5_06 + dat$DD5_07 + dat$DD5_08 + dat$DD5_09
  dat$CMDMax <- dat$CMD07 # add in so not removed below
  dat$DDgood <- dat$DD5 - dat$DD18
  dat$DDnew <- (dat$DD5_05 + dat$DD5_06 +dat$DD5_07  + dat$DD5_08)  - (dat$DD18_05 + dat$DD18_06 +dat$DD18_07 +dat$DD18_08)
  dat$TmaxJuly <- dat$Tmax07
    dat$Boreal <- ifelse(dat$EMT <=-40, 1, 0)
  # dat$Snow_wt <- ifelse(dat$Tave_wt >0, 0, dat$PAS_wt)
  # dat$Snow_at <- ifelse(dat$Tave_at >0, 0, dat$PAS_at)
  # dat$Snow_sp <- ifelse(dat$Tave_sp >0, 0, dat$PAS_sp)
  # dat$Snow <- dat$Snow_wt + dat$Snow_at + dat$Snow_sp
  #dat$CCI <- (1.7*(dat$MWMT - dat$MCMT) / sin(dat$Latitude +10))-14
   dat$CCI <- (1.7* (dat$MWMT - dat$MCMT/sin(dat$Latitude +10)))-14
    dat$KOI <- (100*(dat$Tave10 - dat$Tave04))/ (dat$MWMT - dat$MCMT)

  return(dat)
}



```

Points from a 8km hex grid of western north america are generated in R and submitted to ClimateNA to extract annual and seasonal variables for the historic normal period (1961-90) and an ensemble future climate scenario (rcp45 2040-2070). These data sets are combined. Several additional climate variables are generated including several monthly climate sums for precipitation and growing degree days. All other monthly variables are removed. A winter rechange moisture deficit was calculated and summed with the climatic moisture deficit to account for regions that begin the growing season in soil moisture deficit.

```{r read in Grid Climate Data}
X <- fread("./inputs/WNA_8000m_HexPts_BGC_Normal_1961_1990MSY.csv")#read in historic period
# # ##remove plot id information
X1 <- X %>% dplyr:: select(-ID1, -Latitude, -Longitude, -Elevation)
# 
# ####Generate additional variables
X1 <- addVars(X1)
modelvars <- colnames(X1)[-1]
```

Create different model variable sets
v1=all
v2 = no months
v3 = Biological Variables
vs4 = near zero variance and >.90 correlation removed
vs5 = 16var
vs6 = 35var
vs7 = new Will's variables
vs8 = 35 var
vs9 = reduced 35 for biologial variable reduction
vs10+ = testing effects and final set

```{r create variable sets}
vs1 <- modelvars
month <- c("01", "02", "03", "04", "05", "06","07", "08", "09", "10", "11", "12")
vs2 <- modelvars[-grep(paste(month, collapse = "|"),modelvars)]
badvars <- "TD"
vs2 <- vs2[!vs2 %in% badvars]
vs3 <- c("MAT","MCMT","MWMT","TD","MAP","MSP","AHM","SHM","DD_0","DD5","NFFD","bFFP","eFFP",
         "PAS","EMT","Eref","CMD","Tave_wt","Tave_sm","PPT_wt","PPT_sm")
# X1_no_nzv_pca <- preProcess(X1[,!"BGC"], method = c( "nzv"))
# X1temp <- data.table::copy(X1)
# X1temp[,X1_no_nzv_pca$method$remove := NULL]
# X1temp <- X1temp %>%  na_if(-9999.0) %>% drop_na()
# X_corr <- cor(X1temp[,!"BGC"])
# X1_corr <- findCorrelation(X_corr, cutoff = .90, names = TRUE, verbose = F)
# X1temp[,(X1_corr) := NULL]
# vs4 <- modelvars[modelvars %in% colnames(X1temp)]
vs5 <- read.csv("./outputs/Model_16Vars.csv") ### from original model
vs5 <- vs5$x
vs6 <- read.csv("./outputs/35Var_WNABGCv11.csv") ### from current and future = uncorrelated
vs6 <- vs6$rowname
vs7 <- c("MWMT","MSP","SHM","DD5_sp", "DD5_sm","NFFD","bFFP",
         "PAS","Eref","CMDMax","Tmin_wt","Tave_sm", "CMD.total")
vs8 <- c("Tave_at","MWMT", "DD5_at",   "DD5_sm",   "DD5_sp", "DD5_wt", # 35 variable
            "Eref_sm",   "Eref_sp",
         "Tmin_wt", "MCMT","Tmin_sp",
         "PPT_JAS",  "PPT_MJ",  "PPT_sm","MSP","PPT_sp",
         "CMD_sp","CMD_at", "CMD.def", "CMDMax","CMD","CMD.total","SHM","AHM",
          "Tmax_sp", "Tmax_sm", 
          "bFFP",  "eFFP",  "NFFD", 
          "PAS_wt",  "PAS_at",  "PAS_sp", "PPT_wt","PPT.dormant","PAS_sm")

vs9 <- c( "DD5_at",   "DD5_sm",   "DD5_sp", "DD5", #"DD5_wt","Tave_at",
            "Eref_sm",   "Eref_sp", #"TD",
           #"Tmin_sp", "MCMT", #Tmin_wt","EMT",
         "PPT_sm","MSP","PPT_sp",# "PPT_JAS",  "PPT_MJ", 
         "CMD_sp","CMDMax","CMD.total", #"CMD","CMD_at",
         "SHM","AHM",# "CMD.def",
          "Tmax_sp", "Tmax_sm", "MWMT",
          "bFFP",  "FFP",  "NFFD", "NFFD_sp", 
          "PAS_wt",  "PAS_at",  "PAS_sp")#, "PPT_wt","PPT.dormant","PAS_sm") 


vs10 <- c("MWMT", "DD5_at",   "DD5_sm",   "DD5_sp", "DD5", #"DD5_wt",#"Tave_at",
            "Eref_sm",   "Eref_sp", 
      #"EMT", #"MCMT",  #"tmin07", # "Tmin_wt", "Tmin_sp",
         "PPT_sm","MSP","PPT_sp", #"Snow", "PAS", # "PPT_JAS",  "PPT_MJ",  
         "CMD_sp", "CMDMax","CMD.total","SHM","AHM",#,"CMD_at", "CMD.def""CMD",
          "Tmax_sp", "Tmax_sm")#, #, "DD18_07", # "tmax07", 
          #"NFFD_wt")#, "KOI", "CCI") 
        #"PAS_sp", "PAS_sm", "PAS_wt",  "PAS_at", "PPT_wt", "Tave_wt", "Tave_at", "Tave_sp",
        #"Latitude", "Longitude", "Elevation") #"bFFP",  "eFFP", 


vs11 <- c("CMD.total", "CMD_sp", "CMDMax",  "SHM") ###"Create drought model  

vs12 <- c("DD5", "DD18","PAS", "DD18_sm", "Boreal","NFFD", "EMT", "MCMT", "Tmin07", "Tmin_wt", "Tmin_sp") ###Create Temperate vs Boreal

vs13 <- c("Tmin_sp","Tmin_sm", "Tmin_at", "Tmin_wt", "Tmax_sp", "Tmax_sm", "Tmax_at", "Tmax_wt", "DD5", "DD5_sm", "Tave07") ### Alpine parkland

vs14 <- c("TD", "NFFD", "DD5", "FFP", "Tmin_wt") ### Coast vs Interior

vs15 <- c("MSP", "NFFD", "DD5_sp", "CMD.total") ### Coast vs Interior
      #     #
vs16 <- c("MWMT","MSP","SHM","DD5_sp", "DD5_sm","NFFD","bFFP",
         "PAS","Eref","CMDMax","Tmin_wt","Tave_sm", "CMD.total","CMD","DD5","MCMT")
vs17 <- c("CMD","DD5","MCMT","Tmin_wt","MSP")
# control <- rfeControl(functions = rfFuncs, method = "cv", number = 10)
# X1temp <- X1 %>%  na_if(-9999.0) %>% drop_na()
# X1temp[,BGC := as.factor(BGC)]
# res <- rfe(X1temp[,!"BGC"], X1temp[,BGC], sizes = seq(5,50,by = 5), rfeControl = control)
```


```{r load and prep training data}
#X0 <- fread("./inputs/WNAv12_2000m_Pts_Normal_1961_1990MSY.csv")
#X0 <- fread("./inputs/WNA_4k_HexPts_BGC_Normal_1961_1990MSY.csv")
X0 <- fread("./inputs/WNAv12_2000m_Pts_Normal_1961_1990MSY.csv")

X0 <- X0 %>% select(-ID1) %>% rename(ID1 = V1) %>% dplyr::rename(BGC = ID2)#tibble::rownames_to_column("ID1") #
#X0 <- fread("./inputs/BC_2000m_Pts_Normal_1961_1990MSY_reduced.csv")
#X0 <- fread("./inputs/WNA_4k_HexPts_BGC_Normal_1961_1990MSY.csv")
Xreduce <- fread("./ClosestCentroid35Var.csv")
Xreduce <- Xreduce %>% dplyr::select(-BGC)
Xjoin <- left_join(Xreduce, X0,  by = "ID1") 
XDistrict <- fread("./WNA_2k_ID_District.csv") %>% select(-Latitude,-Longitude)
Xjoin <- left_join(Xjoin, XDistrict)
XTSA <- fread("./inputs/WNA_2k_ID_TSA.csv" )%>% select(-Latitude,-Longitude)
Xjoin <- left_join(Xjoin, XTSA)
Xjoin <- Xjoin %>% filter (!(BGC == 'CWHws2' & Dist_Code == 'DSS')) %>%
                      filter (!(BGC == 'CWHws2' & Dist_Code == 'DND')) %>% 
                        filter (!(BGC == 'ICHmc2' & Dist_Code == 'DND')) %>% 
                    filter (!(BGC == 'ICHmc2' & Dist_Code == 'DSS')) %>% 
                     filter (!(BGC == 'SBSmc2' & Dist_Code == 'DNI')) %>%  
                    filter (!(BGC == 'CWHms1' & Dist_Code == 'DCS')) %>% 
                         filter (!(BGC == 'CWHms1' & Dist_Code == 'DCC')) %>% 
                       filter (!(BGC == 'CWHds1' & Dist_Code == 'DCC')) %>% 
                         filter (!(BGC == 'IDFdk1' & Dist_Code == 'DOS'))                    
##---------Create BGC+District units
      #Xjoin <- Xjoin %>% unite(BGC, c("ID2","Dist_Code"))
      #X0 <- Xjoin %>%  filter(Same == "TRUE") %>% dplyr::select(-ID1,-Latitude, -Longitude,-Elevation, -Same, -Centroid, -MD, -TSA)
    ### Output file to look at excluded training points in QGIS
    #X0_site <- Xjoin %>% select(ID1, ID2, Latitude, Longitude, Same, Centroid, MD) 
    #write.csv (X0_site, "./outputs/2kmPoints_outliers2.csv")
#
X0 <- Xjoin  %>% filter(Same == "TRUE") %>% dplyr::select(-ID1,-Latitude, -Longitude,-Elevation, -Same, -Centroid, -MD, -TSA, -Dist_Code)## %>% dplyr::rename(BGC = ID2)

#X0 <- X0 %>% dplyr::rename(BGC = ID2) #%>% dplyr::select(-V1, -ID1,-Latitude, -Longitude,-Elevation)

#BGC_names <- fread("D:/CommonTables/BGC_maps/WNAv12_BGClist.csv")

###removing coast transition mapping/climate surface problems

      #bad_bgcs <- c('MSdm3', 'ESSFdc3', 'ICHmk2', 'IDFdc', 'SBSun')
###------------reduced 1 Coast Tranistion Units occuring in the interior
X0_removed <- X0 %>% filter (!(BGC == 'CWHws2' & MCMT <= -7)) %>%
                        filter (!(BGC == 'CWHds1' & MCMT <= -4)) %>%
                        filter (!(BGC == 'CWHds2' & MCMT <= -6)) %>%
                         filter (!(BGC == 'CWHms2' & MCMT <= -6)) %>%
                           filter (!(BGC == 'CWHws2' & MCMT <= -6.5)) %>%
                          filter (!(BGC == 'ICHmc2' & MCMT <= -8.5)) %>%
                          filter (!(BGC == 'ICHmc1' & MCMT <= -9.5)) %>%
                          filter (!(BGC == 'MHmm2' & MCMT <= -9)) %>%
  #-------------------reduced 2 droughty BGCs with dirty CMD
                           filter (!(BGC == 'IDFdk1' & CMD <= 290)) %>%
                            filter (!(BGC == 'IDFxm' & CMD <= 320)) %>%
                            filter (!(BGC == 'IDFdk2' & DD5 <= 1000)) %>%
                              filter (!(BGC == 'ESSFdc2' & CMD <= 130)) %>%
  #-------------------reduced 3 BGCs around Kamloops with very mixed mapping

                            #filter (!(BGC %in% bad_bgcs)) %>%
  #-------------------reduce 4  Variable Kamloops units
                            filter (!(BGC == 'MSxk2' & MSP >= 225)) %>%
                            filter (!(BGC == 'IDFxh2' & MSP >= 200)) %>%
#                            filter (!(BGC == 'ESSFdc2' & Longitude >= -120.5)) %>%
                              filter (!(BGC == 'MSdm2' & MSP >= 215))

  X0 <- X0_removed

  # bgcs <- BGC_names

# bad_bgcs <- c('MSdm3', 'ESSFdc3', 'ICHmk2')
# #------------reduced 1 Coast Tranistion Units
# X0_removed <- X0 %>% filter (!(BGC == 'CWHws2' & MCMT <= -7)) %>% 
#                         filter (!(BGC == 'CWHds1' & MCMT <= -4)) %>% 
#                         filter (!(BGC == 'CWHds2' & MCMT <= -6)) %>%   
#                          filter (!(BGC == 'CWHms2' & MCMT <= -6)) %>%  
#                            filter (!(BGC == 'CWHws2' & MCMT <= -6.5)) %>% 
#                           filter (!(BGC == 'ICHmc2' & MCMT <= -8.5)) %>% 
#                           filter (!(BGC == 'ICHmc1' & MCMT <= -9.5)) %>% 
#                           filter (!(BGC == 'MHmm2' & MCMT <= -9)) %>% 
#   #-------------------reduced 2
#                             filter (!(BGC == 'IDFdk1' & DD5 >= 900)) %>% 
#                             filter (!(BGC == 'IDFxm' & DD5 >= 1150)) %>% 
#                             filter (!(BGC == 'IDFdk2' & DD5 >= 1000)) %>% 
#   #-------------------reduced 3 BGCs around Kamloops with very mixed mapping
# 
#                             filter (!(BGC %in% bad_bgcs)) %>% 
#   #-------------------reduce 4  Variable Kamloops units
#                             filter (!(BGC == 'MSxk2' & MSP >= 225)) %>% 
#                             filter (!(BGC == 'IDFxh2' & MSP >= 200)) %>%                               
#                             filter (!(BGC == 'ESSFdc2' & Longitude >= -120.5)) %>% 
#                               filter (!(BGC == 'MSdm2' & CMD <= 225))
# 
#   X0 <- X0_removed
# bgcs <- BGC_names

# X0 <- inner_join(X0, BGC_names) %>% select(-BGC) 
# X0$BGC <- X0$Zone
#X0$BGC <-   dplyr::recode(X0$BGC, MS = "SBS_MS", SBS = "SBS_MS", SBPS = "SBS_MS", BWBS = "SBS_MS", SWB = "SBS_MS", ESSF= "SBS_MS",.default = "Not_Boreal")
#X0$BGC <-   dplyr::recode(X0$BGC, ICH = "ICH", default = "Not_ICH")

#X0$BGC <-   dplyr::recode(X0$BGC, BAFA = "Alpine",IMA = "Alpine", CMA = "Alpine", .default = "Not_Alpine")
#X0$BGC <-   dplyr::recode(X0$BGC, CWH = "Coast", CDF = "Coast", CRF = "Coast", .default = "Interior")                      
#X0$BGC <-   dplyr::recode(X0$BGC, IDF = "IDF_PP", BG = "IDF_PP", PP = "IDF_PP")
#bgcs <- c("IDFww1", "BWBSuf", "BWBSlb", "ICHmc1a", "MHun", "ESSFab","ESSFmkp")
#bgcs <- c("IDFxh2")#"MHmm2", "CWHds1", 
#X0 <- X0 %>% filter (BGC %in% bgcs)

X0[X0 == -9999] <- NA
X0 <- X0[complete.cases(X0),]
X0 <- addVars(X0)


xx <- X0 %>% filter(BGC == "IDFdk1")

X0[,ID1 := NULL]
setnames(X0, old = c("V1","ID2"), new = c("ID1","BGC"))
IDVars <- X0[,.(ID1,Latitude,Longitude)]
X0 <- X0[,c("ID1","BGC",vs8), with = F] ## select the variable set here

# dists <- st_read(dsn = "./inputs/TSA_Boundaries.gpkg")
# pnts <- st_as_sf(IDVars, coords = c("Longitude","Latitude"), crs = 4326)
# dists <- dists["TSA_NUMB_1"]
# dists <- st_transform(dists,4326)
# pnts2 <- st_join(pnts,dists,left = F)
# pnts2 <- st_drop_geometry(pnts2)
# pnts2 <- as.data.table(pnts2)
# IDVars[pnts2, TSA := i.TSA_NUMB_1, on = "ID1"]
# fwrite(IDVars,"WNA_2k_ID_TSA.csv")
# no_nzv <- preProcess(X0[,!c("BGC","ID1")], method = c( "nzv"))
# no_nzv$method$remove
# no_corr <- findCorrelation(cor(X0[,!c("BGC","ID1")]), cutoff = .9, names = TRUE, verbose = F)
# toRemove <- unique(c(no_nzv$method$remove,no_corr))
# X0[,(toRemove) := NULL]


# X0 <- addInteractVars(X0)
# X0 <- X0 %>% select(-Tmin_wt, -MCMT, -PPT_JAS, -PPT_MJ, -PPT_sm, -PPT_wt, -PPT.dormant, -PPT_sp, -MSP,  -PAS_sm )

#X0 <- as.data.frame(X0)

count_tp <- X0 %>% count(BGC)
X0 <- X0 %>% select(BGC, everything())
Xoutlier <- X0 %>% select(BGC, CMD.total, DD5, MSP, NFFD)%>% mutate_if(is_integer,as.numeric) 
#X0 <- X0 %>% mutate_if (is.factor, as.character) %>% mutate_if(is_integer,as.numeric) %>% mutate_if(is.character, as.factor)

### r remove outlier points}
removeOutlier <- function(dat, alpha){
  temp <- as.data.table(dat)
  temp[,MD := mahalanobis(.SD,center = colMeans(.SD),cov = pseudoinverse(cov(.SD)), inverted = T), by = BGC]
  ctf <- qchisq(1-alpha, df = ncol(temp)-1)
  temp <- temp[MD < ctf,]
  return(temp)
}

#Xoutlier2 <- Xoutlier %>% filter(BGC == "IDFdk1")
#XX1 <- removeOutlier(Xoutlier, alpha = .01, numIDvars = 1) ###set alpha for removal of outlieres (2.5% = 3SD)
###outlier function requires 1 column labeled BGC and no other id columns
XX <- removeOutlier(Xoutlier, alpha = 0.4) ###set alpha for removal of outlieres (2.5% = 3SD)
X0 <- as.data.table(XX)

count_tp <- X0 %>% count(BGC)
X0 <- X0 %>% select(BGC, everything())

#X0 <- X0 %>% mutate_if (is.factor, as.character) %>% mutate_if(is_integer,as.numeric) %>% mutate_if (is.character, as.factor)

### r remove outlier points}

removeOutlier <- function(dat, alpha, numIDvars){
  out <- foreach(curr = unique(as.character(dat$BGC)), .combine = rbind) %do% {
    temp <- dat[dat$BGC == curr,]
    md <- tryCatch(mahalanobis(temp[,-c(1:numIDvars)],
                               center = colMeans(temp[,-c(1:numIDvars)]),
                               cov = cov(temp[,-c(1:numIDvars)])), error = function(e) e)
    if(!inherits(md,"error")){
      ctf <- qchisq(1-alpha, df = ncol(temp)-1)
      outl <- which(md > ctf)
      cat("Removing", length(outl), "outliers from",curr, "; ")
      if(length(outl) > 0){
        temp <- temp[-outl,]
      }
    }
    temp

  }
  return(out)
}

XX <- removeOutlier(X0, alpha = .011, numIDvars = 1) ###set alpha for removal of outlieres (2.5% = 3SD)
X0 <- as.data.table(XX)

X0[,Num := .N, by = .(BGC)]
X0_OK <- as.data.table(X0[Num > 100 & Num < 500,])
X0[,LogNum := log(Num,base = 10)]
X0[Num <= 100, New := scales::rescale(LogNum, to = c(50,100))]
X0[Num >= 500, New := scales::rescale(LogNum, to = c(500,1000))]
X0[,New := as.integer(New)]

X0temp <- as.data.table(X0[Num >= 500,])
X0temp$BGC <- as.factor(X0temp$BGC)
cl <- makeCluster(detectCores()-2)
registerDoParallel(cl)

X0_big <- foreach(unit = unique(X0temp$BGC), .combine = rbind,
                  .packages = c("clhs","data.table")) %dopar% {
  cat("Processing",unit,"\n")
  temp <- X0temp[BGC == unit,]
  num <- temp$New[1]
  ##nz <- nearZeroVar(temp, names=T) ##do we need this?
  lhs <- clhs(temp[,!c("BGC","Num","LogNum","New")],size = num, iter = 1000, use.cpp = T, simple = F)
  res <- lhs$sampled_data
  res$BGC <- unit
  res
}

X0temp <- as.data.table(X0[Num <= 100,])# %>% filter (!(BGC == "ESSFxh_WA" | BGC == "IMAun_UT" | BGC == "MHun" ))
X0_small <- X0temp %>% select(-New, -Num, -LogNum)

 foreach(unit = unique(X0temp$BGC), .combine = rbind, .packages = c("clhs","data.table")) %do% {
  cat("Processing",unit,"\n")
  temp <- X0temp[BGC == unit,] %>% mutate_if(is.integer,as.numeric)
  temp$BGC <- as.numeric(as.factor(temp$BGC))
  num <- (temp$New[1])/temp$Num[1]
  temp <- as.data.table(temp)
  tempSMOTE <- smotefamily::SMOTE(temp[,!c("BGC","Num","LogNum","New")],
                             temp$BGC, K = 2, dup_size = ceiling(num))
  newCases <- temp$New[1] - temp$Num[1]
  synData <- tempSMOTE$syn_data
  synData <- synData[sample(1:nrow(synData), size = newCases, replace = F),]
  synData$class <- NULL
  synData$BGC <- unit
  synData
}

X0_OK[,Num := NULL]
X0_OK<- X0_OK %>% select(-New,  -LogNum)
XAll <- rbind(X0_small,X0_OK,X0_big)
#XAll <- X0  ###unalanced data set
XAll <- as.data.table(XAll)

XAll[,BGC := as.factor(BGC)]
XAll$BGC  <- droplevels(XAll$BGC)

```

The new rebalanced training set is 310 668 points. This training set is submitted to ranger to generate a final climate model of BGCs for western north america.

```{r mahalanobis distance}
set.seed(234589)
cores <- parallel::detectCores()
cores

#temp <- X0[,!"ID1"]
#temp <- temp[,!c("DD5_wt","PAS_sm")]
#temp <- temp[,lapply(.SD,function(x){range(x)[2] - range(x)[1]}),by = BGC]
# zeroVar <- temp[CMD == 0,as.character(BGC)]
# X0[,CMD := as.numeric(CMD)]
# X0[as.character(BGC) %chin% zeroVar, CMD := jitter(CMD, amount = 0.5)]

BGC_Nums <- X0[,.(Num = .N), by = BGC]
BGC_train <- X0[!BGC %in% BGC_Nums[Num < 35, BGC],]##remove BGCs with low numbers of points

# This section was to try and remove linear dependance, but with pseudoinverse I don't think we need it
# BGC_train[BGC_train == 0] <- 0.1
# temp <- BGC_train[,lapply(.SD,function(x){range(x)[2] - range(x)[1]}),by = BGC, .SDcols = -"ID1"]
# #BGC_train <- BGC_train[,lapply(.SD,as.numeric), .SDcols = -c("ID1","BGC"), by = .(ID1,BGC)]
# for(b in colnames(BGC_train)[-(1:2)]){
#   zeroVar <- temp[get(b) == 0,as.character(BGC)]
#   BGC_train[BGC %in% zeroVar, (b) := jitter(get(b), amount = 3)]
# }

##proximity of centroids to each other
library(StatMatch)
library(corpcor)
BGC_Centroids <- BGC_train[,lapply(.SD,mean,na.rm = T),by = BGC, .SDcols = -c("ID1")]
BGC_Centroids[BGC_Centroids == 0] <- 0.1
cent_dist <- mahalanobis.dist(BGC_Centroids[,!"BGC"])
rownames(cent_dist) <- BGC_Centroids$BGC
colnames(cent_dist) <- BGC_Centroids$BGC
hist(cent_dist)
#cent_dist[lower.tri(cent_dist, diag = T)] <- NA
cent_dt <- as.data.table(cent_dist)
cent_dt[,Unit1 := rownames(cent_dist)]
cent_dt <- melt(cent_dt, id.vars = "Unit1")
cent_dt <- cent_dt[!is.na(value),]
#cent_dt <- cent_dt[value < 0.5,]
setnames(cent_dt, c("Unit1","Unit2","MahDist"))
fwrite(cent_dt, "CentroidDistAllUnits_12var.csv")


##simple mahalanobis distance for mean and var
BGC_train[,mahDist := mahalanobis(.SD, center = colMeans(.SD), cov = pseudoinverse(cov(.SD)), inverted = T), 
          .SDcols = -c("ID1"), by = BGC]
Mah_Summ <- BGC_train[,.(Mean = mean(mahDist), Var = sd(mahDist), Quant = quantile(mahDist,0.9)), by = BGC]
Mah_Summ[BGC_Nums, NumPoints := i.Num, on = "BGC"]
fwrite(Mah_Summ,"MD_Var12.csv")
#BGC_train <- BGC_train[mahDist < 5,]

##MD for each points compared to each centroid to determine which points are closer to a different centroid
##Note this takes a while for the 2km dataset
BGCs <- unique(as.character(BGC_train$BGC))
data.table::setkey(BGC_train,BGC)
distAll <- BGC_train[,.(ID1,BGC)]
pb <- txtProgressBar(min = 0, max = length(BGCs),initial = 0)
i = 0
for(b in BGCs){
  i = i+1
  setTxtProgressBar(pb,i)
  curr <- BGC_train[BGC == b,!c("ID1","BGC")]
  centroid <- colMeans(curr)
  covInv <- pseudoinverse(cov(curr))
  distAll[,(b) := mahalanobis(BGC_train[,!c("ID1","BGC")],
                              center = centroid,cov = covInv,inverted = T)]
}

##summarise
mahAll <- melt(distAll, id.vars = c("ID1","BGC"))
setnames(mahAll, old = c("variable","value"), new = c("Centroid","MD"))
setkey(mahAll,ID1)
minDist <- mahAll[order(MD),.SD[1L],by = ID1]
minDist[,Same := BGC == Centroid]
table(minDist$Same)
fwrite(minDist,"ClosestCentroid35Var.csv")

##join with training data
BGC_train[minDist, Include := i.Same, on = "ID1"]
fwrite(BGC_train,"FlaggedTrainingSet.csv")

##create geopackage
library(sf)
pnts <- IDVars[minDist, on = "ID1"]
pnts <- st_as_sf(pnts, coords = c("Latitude","Longitude"), crs = 4326)
pnts <- st_transform(pnts, 3005)
st_write(pnts, dsn = "TrainingPoints_MD35Var.gpkg")
#minDist <- mahAll[, .SD[which.min(MD)], by = ID1]

# BGCs <- unique(as.character(BGC_train$BGC))
# mahDist <- foreach(b = BGCs, combine = rbind) %do% {
#   cat(b," ")
#   temp <- BGC_train[BGC == b,!c("ID1","BGC")]
#   centroid <- colMeans(temp)
#   matInv <- pseudoinverse(cov(temp))
#   mdist <- mahalanobis(temp,center = centroid, cov = matInv, inverted = T)
# }


```

``` {r }
X0temp <- as.data.table(X0[Num > 500,])
X0temp$BGC <- as.factor(X0temp$BGC)
cl <- makeCluster(detectCores()-2)
registerDoParallel(cl)

X0_big <- foreach(unit = unique(X0temp$BGC), .combine = rbind,
                  .packages = c("clhs","data.table")) %dopar% {
  cat("Processing",unit,"\n")
  temp <- X0temp[BGC == unit,]
  num <- temp$New[1]
  #nz <- nearZeroVar(temp, names=T) ##do we need this?
  lhs <- clhs(temp[,!c("BGC","Num","LogNum","New")],size = (num -1), iter = 1000, use.cpp = T, simple = F)
  res <- lhs$sampled_data
  res$BGC <- unit
  res
}

X0temp <- as.data.table(X0[Num <= 100,])# %>% filter (!(BGC == "ESSFxh_WA" | BGC == "IMAun_UT" | BGC == "MHun" ))
#X0_small <- X0temp %>% select(-New, -Num, -LogNum)

 foreach(unit = unique(X0temp$BGC), .combine = rbind, .packages = c("clhs","data.table")) %do% {
  cat("Processing",unit,"\n")
  temp <- X0temp[BGC == unit,] %>% mutate_if(is.integer,as.numeric)
  temp$BGC <- as.numeric(as.factor(temp$BGC))
  num <- (temp$New[1])/temp$Num[1]
  temp <- as.data.table(temp)
  tempSMOTE <- smotefamily::SMOTE(temp[,!c("BGC","Num","LogNum","New")],
                             temp$BGC, K = 2, dup_size = ceiling(num))
  newCases <- temp$New[1] - temp$Num[1]
  synData <- tempSMOTE$syn_data
  synData <- synData[sample(1:nrow(synData), size = newCases, replace = F),]
  synData$class <- NULL
  synData$BGC <- unit
  synData
}

X0_OK[,Num := NULL]
#X0_OK<- X0_OK %>% select(-New,  -LogNum)
XAll <- rbind(X0_small,X0_OK,X0_big)
#XAll <- X0  ###unalanced data set
XAll <- as.data.table(XAll)
=======
```{r setup tidymodel}

BGC_recipe <- recipe(BGC ~ .,  data = BGC_train) %>%
    step_center(all_predictors()) %>%
  step_scale(all_predictors())

BGC_model <- rand_forest(trees = 101, min_n = 10) %>%# specify that the model is a random forest
  #set_args(mtry = tune()) %>% specify that the `mtry` parameter needs to be tuned
  set_engine("randomForest", num.threads = (cores-2)) %>%  #, importance = "impurity, ) %>% select the engine/package that underlies the model
   set_mode("classification")

 # set the workflow
BGC_workflow <- workflow() %>%
  add_recipe(BGC_recipe) %>%
  add_model(BGC_model)
#
## build model for prediction
BGCmodel.tidy <- BGC_workflow %>% fit(BGC_train)
gc()

BGCmodel.var <- pull_workflow_fit(BGCmodel.tidy)$fit

###Variable importance
varimp <- as.data.frame(BGCmodel.var$importance)
covcount <- count(varimp)
BGCmodel <- BGCmodel.tidy
save(BGCmodel.tidy,file= paste0("./BGC_models/WNAv12_Zone_", covcount, "_Var_tidyrf1.Rdata"))

```

```{r build final ranger model with larger training point size, cache = TRUE, echo=FALSE}

#X0[,BGC := as.factor(BGC)]
#XAll2 <- X0# %>% select(-MCMT, -Tmin_wt)
BGCmodel2 <- ranger(BGC ~ ., data = XAll,
                           num.trees = 101,  seed = 12345,
                            splitrule =  "extratrees", #""gini",
                            always.split.variables = c("DD5","CMD.total", "MSP"), #,
                            #mtry = 8,
                          #max.depth = .5,
                    #min.node.size = 2,
                           importance = "impurity", write.forest = TRUE, classification = TRUE)

DF <- as.data.frame(BGCmodel2$variable.importance) %>% tibble::rownames_to_column() %>% rename(w = 1, v = 2)
covcount <- count(DF)
 ggplot(DF, aes(x=reorder(w,v), y=v,fill=v))+ 
  geom_bar(stat="identity", position="dodge")+ coord_flip() +
   ylab("Variable Importance")+
   xlab("")+
   ggtitle("Information Value Summary")+
   guides(fill=F)+
   scale_fill_gradient(low="red", high="blue")
 
save(BGCmodel2,file= paste0("./BGC_models/WNAv12_Subzone_", covcount, "_Var_ranger_MDreduced3_DropSomeDistricts_alwayssplit.Rdata"))
test <- treeInfo(BGCmodel2,1)

#X0[,BGC := as.factor(BGC)]
# XAll2 <- X0# %>% select(-MCMT, -Tmin_wt)
# BGCmodel2 <- ranger(BGC ~ ., data = XAll,
#                            num.trees = 101,  seed = 12345,
#                             splitrule =  "extratrees", #""gini",
#                             always.split.variables = c("DD5_sp","CMD.total", "MSP"), #,
#                             #mtry = 8,
#                           #max.depth = .5,
#                     #min.node.size = 2,
#                            importance = "impurity", write.forest = TRUE, classification = TRUE)
# 
# DF <- as.data.frame(BGCmodel2$variable.importance) %>% tibble::rownames_to_column() %>% rename(w = 1, v = 2)
# covcount <- count(DF)
#  ggplot(DF, aes(x=reorder(w,v), y=v,fill=v))+ 
#   geom_bar(stat="identity", position="dodge")+ coord_flip() +
#    ylab("Variable Importance")+
#    xlab("")+
#    ggtitle("Information Value Summary")+
#    guides(fill=F)+
#    scale_fill_gradient(low="red", high="blue")
#  
# save(BGCmodel2,file= paste0("./BGC_models/WNAv12_Subzone_", covcount, "_Var_ranger_reduced4.Rdata"))
# test <- treeInfo(BGCmodel2,1)

# test
# #
#=#strata=BGC, sampsize= c(500),do.trace = 10, 
# ##Simple rF model
 # BGCmodel <- randomForest(BGC ~ ., data=XAll,  do.trace = 10, 
 #                         ntree=101, na.action=na.omit, importance=TRUE, proximity=FALSE)
 # 

## tidymodel version


DF <- varimp %>% tibble::rownames_to_column() %>% rename(w = 1, v = 2)

BGCmodel
v <-as.data.frame(BGCmodel.var$importance)

#DF <- v %>% tibble::rownames_to_column() %>% rename(w = rowname, v = 'BGCmodel.var$importance')
# 
#  ggplot(DF, aes(x=reorder(w,v), y=v,fill=v))+
#   geom_bar(stat="identity", position="dodge")+ coord_flip() +
#    ylab("Variable Importance")+
#    xlab("")+
#    ggtitle("Information Value Summary")+
#    guides(fill=F)+
#    scale_fill_gradient(low="red", high="blue")
# #
#  
 
# BGCmodel2 <- randomForest(BGC ~ ., data=XAll,  do.trace = 10,
#                          ntree=101, na.action=na.omit, importance=TRUE, 
#                          proximity=FALSE, keep.forest = T, localImp = T)
# 
# library(randomForestExplainer)
# min_depth_frame <- min_depth_distribution(BGCmodel2)
# explain_forest(BGCmodel2)
# 
# library(reprtree)
# rtree <- ReprTree(BGCmodel2,)
# reprtree:::plot.getTree(BGCmodel2)

# model_vars <- as.data.frame(BGCmodel$variable.importance) %>% tibble::rownames_to_column()
# model_vars <- as.data.frame(BGCmodel$importance) %>% tibble::rownames_to_column()
# covcount <- count(model_vars)
#  fname <- "new_35VAR"
# model = "_rf_6Oct2020"
# write.csv (BGCmodel$variable.importance, file= paste("./outputs/",covcount,"Var_Importance_xtra",model,".csv",sep=""))
# write.csv (BGCmodel$prediction.error, file= paste("./outputs/",covcount,"_Error",model,".csv",sep=""))
# write.csv (BGCmodel$confusion.matrix, file= paste("./outputs/",covcount,"_ConfusionMatrix",model,".csv",sep=""))

```



## For getting metrics

```{r build test models ranger model with larger training point size, cache = TRUE, echo=FALSE}
## tidymodel version with cross validation

# set.seed(234589)
# # split the data into trainng (75%) and testing (25%)
# ## prep data
# BGC_split <- initial_split(XAll, prop = 3/4)
# #BGC_train <- XAll
# BGC_train <- training(BGC_split) %>% droplevels()
# BGC_test <- testing(BGC_split) %>% droplevels()
# BGC_folds <- vfold_cv(BGC_train, v=10)
# ##M recipe
# BGC_recipe <- recipe(BGC ~ .,  data = BGC_train)
# BGC_model <- rand_forest(trees = 71) %>%# specify that the model is a random forest
#   #set_args(mtry = tune()) %>% specify that the `mtry` parameter needs to be tuned
#   set_engine("randomForest") %>%  #,, importance = "impurity" ) %>% select the engine/package that underlies the model
#    set_mode("classification") 
#  
#  # set the workflow
# BGC_workflow <- workflow() %>%
#   add_recipe(BGC_recipe) %>%
#   add_model(BGC_model)
# 
# ##build cross-validated model
# BGCmodel.tidy.cv <- BGC_workflow %>% fit_resamples(BGC_folds)
# metrics_BGC <- collect_metrics(BGCmodel.tidy,cv)
# 
# ## build model for prediction
# tic()
# BGCmodel.tidy <- BGC_workflow %>% fit(BGC_train)
# toc()
# gc()
# BGCmodel.var <- pull_workflow_fit(BGCmodel.tidy)$fit
# 
# ###Variable importance
# varimp <- as.data.frame(BGCmodel.var$importance)
# covcount <- count(varimp)
# BGCmodel.var <-as.data.frame(BGCmodel.var$importance)
# DF <- varimp %>% tibble::rownames_to_column() %>% rename(w = 1, v = 2)
#  
#  ggplot(DF, aes(x=reorder(w,v), y=v,fill=v))+ 
#   geom_bar(stat="identity", position="dodge")+ coord_flip() +
#    ylab("Variable Importance")+
#    xlab("")+
#    ggtitle("Information Value Summary")+
#    guides(fill=F)+
#    scale_fill_gradient(low="red", high="blue")
# # varimport <- BGCmodel.tidy %>% 
# #   pull_workflow_fit() %>% 
# #   vip(num_features = 10)
# 
# ### predict test set
# BGC_testing_pred <- 
#   predict(BGCmodel.tidy, BGC_test) %>% 
#   bind_cols(predict(BGCmodel.tidy, BGC_test, type = "prob")) %>% 
#   bind_cols(BGC_test %>% select(BGC))
# 
# ###look at  accuracy against test set
# 
# BGC_accuracy <- BGC_testing_pred %>%                   # test set predictions
#   accuracy(truth = BGC, .pred_class)
# 
# table(BGC_accuracy)
# 
# BGCmodel <- BGCmodel.tidy
# save(BGCmodel,file= paste0("./BGC_models/WNAv12_Subzone_", covcount, "_Var_tidyrf5.Rdata"))
# 
# model = "_rf_18Oct2020"
# write.csv (BGCmodel$variable.importance, file= paste("./outputs/",covcount,"Var_Importance_xtra",model,".csv",sep=""))
# write.csv (BGCmodel$prediction.error, file= paste("./outputs/",covcount,"_Error",model,".csv",sep=""))
# write.csv (BGCmodel$confusion.matrix, file= paste("./outputs/",covcount,"_ConfusionMatrix",model,".csv",sep=""))

```


##Not yet functioning

``` {r c add region code to BGCs for testing }
# X_test <- fread("./inputs/WNAv12_2000m_Pts_Normal_1961_1990MSY.csv")
# coordinates (X_test) <- ~ Longitude + Latitude
# proj4string(X_test) = CRS("+init=epsg:4326")
# pt.rast <- st_as_sf(X_test) %>% st_transform( crs = st_crs(3005))
# 
# 
# 
#   districts = st_read("D:/CommonTables/BC_AB_US_Shp/ForestRegions.gpkg", layer = "ForestDistricts2") %>% st_as_sf() %>%# filter(ORG_UNIT %in% region) %>%  ## district boundaries
#     st_transform( crs = st_crs(3005)) %>%
#   # st_buffer(., dist = 500) %>%
#   as(., "Spatial")
# 
#    # as(., "Spatial")
# districts$code <- as.factor(districts$ORG_UNIT)
# district.names <- as.data.frame(levels(districts$code))%>% rownames_to_column("ID")
# district.names$ID <- as.integer(district.names$ID)
# 
# districts.rast <- fasterize::fasterize(districts, pt.rast, field = "code", fun = "first")
# plot(wna_bgc.rast)
# ##------------------------------------------------------------------------------------------
# 
# ID2 <- ""
# ID2 <- raster::extract(wna_bgc.rast,wna.pt.rast)
# ID2 <- as.data.frame(ID2) %>% rownames_to_column("ID1")
# #ID2$ID2 <- as.integer(ID2$ID2)
# 
# ID <- left_join (ID2, BGC.names, by = c("ID2" = "factorID")  )
# 
# 
# # wna.pt3 <-  st_coordinates(wna.pt2) 
# # wna.pt3 <- as.data.frame(wna.pt3) %>% rownames_to_column("ID1")
# # wna_cb1 <- left_join(wna.pt3, elev) %>% rename("latitude" = Y, "longitude" = X) 
# wna_cb <- left_join(wna_cb1, ID)
# wna_cb <- wna_cb %>% dplyr::select(-ID2) %>% rename(ID2 = 5)
# 
# Old code
```

###old code

```{r old code}
# addInteractVars <- function(dat){
#     BGC_recipe <- recipe(BGC ~ .,  data = dat) ###cold BGCs (SBS, ESSF)
# interact_2 <- BGC_recipe %>%
#   step_interact(terms = ~ Tmin_wt:DD5_sp)
# interact_2 <- prep(interact_2, training = dat)
# dat <- bake(interact_2,dat)
# 
# BGC_recipe <- recipe(BGC ~ .,  data = dat) ###dry warm (IDF, PP, BG)
# interact_3 <- BGC_recipe %>%
#   step_interact(terms = ~ CMD.total:DD5_sp)
# interact_3 <- prep(interact_3, training = dat)
# dat <- bake(interact_3,dat)
# 
# BGC_recipe <- recipe(BGC ~ .,  data = dat) ### low drought from cold vs low drought from precip
# interact_4 <- BGC_recipe %>%
#   step_interact(terms = ~ CMD.total:MSP)
# interact_4 <- prep(interact_4, training = dat)
# dat <- bake(interact_4,dat)
# 
# BGC_recipe <- recipe(BGC ~ .,  data = dat) ## 
# interact_5 <- BGC_recipe %>%
#   step_interact(terms = ~ CMD.total:MSP:DD5_sp)
# interact_5 <- prep(interact_5, training = dat)
# dat <- bake(interact_5,dat)
# 
# BGC_recipe <- recipe(BGC ~ .,  data = dat)
# interact_1 <- BGC_recipe %>%
#   step_interact(terms = ~ CMD.total:MSP:DD5_sp:Tmin_wt)
# interact_1 <- prep(interact_1, training = dat)
# dat <- bake(interact_1,dat)
# 
# BGC_recipe <- recipe(BGC ~ .,  data = dat) ##Coast vs interior Continentaliy plus growing 
# interact_6 <- BGC_recipe %>%
#   step_interact(terms = ~ CCI:DD5_sp)
# interact_6 <- prep(interact_6, training = dat)
# dat <- bake(interact_6,dat)
#   #   
#   #   (dat$PAS10 + dat$PAS11  + dat$PAS12 + dat$PAS01 + dat$PAS02 + dat$PAS03 + dat$PAS04 ) - ((dat$DD5_11 + dat$DD5_wt  + dat$DD5_03) * 50)
#   # dat$Snow <- ifelse (dat$Snow <0, 0, dat$Snow)
#  return(dat)
#}

```


```{r predict 2km hex}
#load( "./outputs/WNAv11_9_VAR_SubZone_ranger.Rdata")
# vars <- as.data.frame(BGCmodel$variable.importance)
# vars <- row.names(vars)
# X1 <- fread("./inputs/WNA_2k_HexPts_BGC_Normal_1961_1990MSY.csv", stringsAsFactors = FALSE, data.table = FALSE)
# X1_loc <-  X1 %>% dplyr::select(ID1, Latitude, Longitude, Elevation)
# X1 <- addVars(X1)
# X1 <- X1 %>% dplyr::select(ID1, vars)
# #X1 <- fread("D:/CommonTables/HexGrids/WNAv11_HexGrid_800m_Cleaned_Normal_1961_1990MSY.csv", stringsAsFactors = FALSE, data.table = FALSE)
# 
# X1_loc$preds <- predictions(predict(BGCmodel, data = X1[-c(1)]))
# #X.pred2 <- cbind(X1_loc, X.pred$predictions)
# fwrite(X1_loc, "BGC_NormalPeriod_Predicted_20var_new.csv")
# dem <- raster(df)
# projection(dem) <- P4S.albers
# par(mfrow=c(1,1))
# plot(dem)
# sum(!is.na(values(dem)))

```


```{r build other ML models}

# Test models between all training points and where outliers removed
# set.seed(123321)
# coreNo <- makePSOCKcluster(detectCores() - 1)
# registerDoParallel(coreNo, cores = detectCores() - 1)
# Cores <- as.numeric(detectCores()-1)
# ctrl <- trainControl(method = "cv", number = 10,# repeats = 5,
#                      classProbs = FALSE, verboseIter = T,
#                      savePredictions = "final", 
#                      allowParallel = F)
# 
# 
# #BGCmodel_rang <- train(Zone  ~ ., data = X1_Bio1[-2], #for zone
# BGCmodel_subz <- train(BGC  ~ ., data = X1_final2[-1], # for subzone
#                      method = "ranger",
#                      #preProcess = c("center", "scale", "YeoJohnson"), #,tuneGrid = tgrid,
#                       trControl = ctrl,
#                       importance = "impurity")
# 
#  stopCluster(coreNo)
#  gc()
# 
# BGCmodel_subz
# file=paste("./outputs/USA_SubZone_rangercaret_Mar30",".Rdata",sep="")
# save(BGCmodel_subz,file=file)

# ##Simple rF model
# BGCmodel <- randomForest(BGC ~ ., data=X1[-1],  do.trace = 10, 
#                          ntree=151, na.action=na.omit, importance=TRUE, proximity=FALSE)
# 
#  save(BGCmodel,file= "./outputs/WNAv11_23_VAR_rf.Rdata")
# #fname <- "BGCv11_AB_USA_500each_27VAR_SubZone"
# fname <- "WNA_var23"
# model = "_rF_01Jan2019"
# write.csv(BGCmodel$proximity, file= paste("./outputs/", fname,"_Proximity",model,".csv",sep=""))
# write.csv(BGCmodel$importance, file= paste("./outputs/",fname,"_Importance",model,".csv",sep=""))
# write.csv(BGCmodel$err.rate, file= paste("./outputs/",fname,"_Error",model,".csv",sep=""))
# write.csv(BGCmodel$confusion, file= paste("./outputs/",fname,"_ConfusionMatrix",model,".csv",sep=""))
# write.csv(BGCmodel$confusion[, 'class.error'], file= paste("./outputs/",fname,"_Confusion",model,".csv",sep=""))
# VIP <- varImpPlot(BGCmodel, sort=TRUE) 
# varImpPlot(BGCmodel, sort=TRUE) 
# write.csv(VIP, file= paste("./outputs/",fname,"_OBO",model,".csv",sep=""))
# dev.copy(pdf,(paste(model,'"./outputs/VarImpPlot.pdf')))
# dev.off()
# 
#  ###caret package version
# ###Go back to 
#  set.seed(123321)
# coreNo <- makeCluster(6)
# registerDoParallel(coreNo)
#   control <-trainControl(classProbs = TRUE, allowParallel = TRUE) #method = 'cv', number = 2)#, repeats = 3
#    BGCmodel_caret <- caret::train(BGC ~ ., data=X1,  method= "rf", trControl = control)#,tuneLength = 2,, mtry = 3,  ntree=31)
#  print(BGCmodel_caret)
# stopCluster(coreNo)
# # gc()
#save(BGCmodel_caret,file= "./outputs/WNAv11_16VAR_SubZone_caret.Rdata")

```

The preprocessing function from the caret package was used to identify variables with near-zero variance or correlation >0.90 in the combined data set. These variables were removed leaving a final variable set of 20 variables.
```{r reduce variables, warning=FALSE} 
X1_no_nzv_pca <- preProcess(X1[-1], method = c( "nzv")) # DROP variables with near zero variance
X1_no_nzv_pca
X1_test <- dplyr::select(X1, -c(X1_no_nzv_pca$method$remove))
X1_test <- X1_test %>%  na_if(-9999.0) %>% drop_na()
month <- c("01", "02", "03", "04", "05", "06","07", "08", "09", "10", "11", "12")
X2 <- X1_test  %>% dplyr::select(-ends_with(month)) %>% #removes all monthly variables
  dplyr::select(-starts_with("Rad")) %>% ##remove other non-biological variables
  dplyr::select(-starts_with("RH")) #%>%
  # dplyr::select (-contains("DD_0")) %>%
  #  dplyr::select  (-contains("DD18")) %>%
  #  dplyr::select  (-contains("DD_18"))  %>%
  # dplyr::select( -PPT_sp, -FFP, -PAS_sm, -PPT_at, -PAS_at, - MAP, -TD, -MAT)# remove some redundant variables considered undesireable

########Remove near zero variance and highly correlated variables
X2<- X2 %>%  na_if(-9999.0) %>% drop_na() # drop points without complete data

X_corr <- cor(X2[-1])
X1_corr <- findCorrelation(X_corr, cutoff = .90, names = TRUE, verbose = F)
X1_corr2 <- as.data.frame(X1_corr)
X3 <- dplyr::select(X2, -c(X1_corr))
modelvars <- colnames(X3)
modelvars
write.csv(modelvars, "./outputs/74Var_WNABGCv11.csv", row.names = FALSE)
#modelvars <- fread("./outputs/15Var_WNABGCv11.csv")

```

A larger training point set was built from 2 km hex grid of points for western north america. Points are overlayed the WNA Biogeoclimatic map to identify BGC membership.
```{r build training point set, cache = TRUE}
X0 <- fread("./inputs/WNA_2k_HexPts_BGC_Normal_1961_1990MSY.csv")

X0_loc <-  X0 %>% dplyr::select(ID1, Latitude, Longitude, Elevation)

X0 <- addVars(X0)
##split a couple of units N/S with tree species implications
# X0 <- X0 %>% 
#     mutate(
#       BGC = if_else(BGC == "MSab" & Latitude >51.5, "MSab_N", 
#        if_else(BGC == "CWHvm1" & Latitude >52, "CWHvm1_PR", BGC)))


modelvars <- read.csv("./outputs/35Var_WNABGCv11.csv") ### from current and future = uncorrelated
modelvars <- read.csv("./outputs/AllVar_WNABGCv11.csv") ### 260 variables from CLimateNA
modelvars <- read.csv("./outputs/20Var_WNABGCv11.csv") ### YS variable from current 90% uncorrelated and nzv removed
# modelvars <- modelvars %>% filter(!x== "DD_0_at")
# write.csv(modelvars, "./outputs/18Var_WNABGCv11.csv", row.names = FALSE)
modelvars <- read.csv("./outputs/19Var_WNABGCv11.csv") ### 20 Var with TD removed
modelvars <- read.csv("./outputs/18Var_WNABGCv11.csv") ### 20 Var with TD and DD_0_at removed
modelvars <- read.csv("./outputs/74Var_WNABGCv11.csv") ### YS variables
modelvars <- read.csv("./outputs/WHMVar_WNABGCv11.csv") ### picked list with emphasis on controlling variables

X1 <- X0 %>% dplyr::select(ID1, BGC, modelvars$x) %>% drop_na(BGC)



X1$BGC <- as.factor(X1$BGC)


#write.csv(X_corr,"./outputs/22variablecorrelation.csv")
# model = "5.1"
# VarList = c("AHM", "bFFP","CMD.total","DD5_sp","EMT","Eref_sm","EXT","FFP","MCMT","MSP",
#             "PPT_JAS","PPT_MJ","PPT06","SHM","TD","Tmax_sp","Tmin_at","Tmin_sm","Tmin_wt","PAS",
#             "CMD.def","CMDMax","eFFP","Eref09","MAT","PPT07","Tmin_sp")
# List = c("ID1", "BGC")
# X1$BGC  <- as.factor(X1$BGC)
# X1 <- X1[,names(X1) %in% c(List,VarList)]
# modelvars <- read.csv (as.character("./outputs/Final23Var_WNABGCv11.csv"))
# modelvars <- as.character(modelvars$x)

```


```{r mahanolobis distance of BGCs}
X4 <- X1 %>% mutate_if(is.numeric,as.integer) %>% mutate_if(is.factor,as.integer)
mah <- as.data.table(X4)[BGC %in% c(1,2), mahalanobis.dist(CMD_sp), keyby = BGC]

X4 <- as.data.table(X4)
mah.dat <- X4 [BGC, mahalanobis.dist(data.x = X4), keyby = BGC]

```

The 1,175,378 training point population is highly imbalanced between the 362 biogeoclimatic subzone/variants. Geographically large BGCs dominate the machine learning model when raw data set are used in the training set. A partial balancing of the raw training point between classes was applied to counteract this effect. Rebalancing is accomplished by log10 scaling the count of points per BGC and then rescaling the log scale to a range of 200 - 2000 training points per BGC.  Large BGCs with raw training sets larger than the rescaled sample are subsampled using conditioned Latin Hypercube Sampling. Geographically restricted BGCs with fewer than required samples are upsampled using the SMOTE routine.   
```{r resample training points}
#Calculate sample number and rescale to semi-balance the training point set
X2 <- X1#[-1]
#X2 <- X2 %>% filter(BGC != "MHRFmmp_OR", BGC != "ESSFmwp_WA", BGC != "CMAwh")
X2$BGC <- as.factor(X2$BGC)
rownames(X2) <- X2$ID1
countSubzone <- X2 %>% count(BGC)
X2_OK <- countSubzone[(countSubzone$n >250 & countSubzone$n <1000),] # These units have OK samples
X2_OK <- as.character(unique(X2_OK$BGC))
X2_OK2 <- X2[X2$BGC %in% X2_OK,]
X2_OK2 <- droplevels(X2_OK2)

X2_Few <- countSubzone[countSubzone$n <= 250,]## these will need SMOTE additions
X2_Few$logn <- log(X2_Few$n, 10)
X2_Few$new <- as.integer (rescale(X2_Few$logn, to = c(50, 250), from = range(X2_Few$logn, na.rm = TRUE, finite = TRUE)))
X2_Few <-  X2_Few %>% mutate(ratio = new/n)
X2_Few <- droplevels(X2_Few)

X2_LHC  <- countSubzone[countSubzone$n >= 1000,]### these will be reduced in number
X2_LHC$logn <- log(X2_LHC$n, 10)
X2_LHC$n <- as.integer (rescale(X2_LHC$logn, to = c(1000, 2000), from = range(X2_LHC$logn, na.rm = TRUE, finite = TRUE)))
X2_LHC <- droplevels(X2_LHC)
#countSubzone$sample <- ifelse(countSubzone$rs > countSubzone$n, countSubzone$n, countSubzone$rs )
allUnits <- unique(X2$BGC)
X2 <- na.omit(X2)
X2$BGC <- as.factor(X2$BGC)
# & (countSubzone$n >= 500),] ##these have enough samples

#X2_LHC  <- countSubzone[countSubzone$n > countSubzone$rs,] ### these will be reduced in number
#X2_LHC <- X2_OK
OKUnits <- unique(X2_OK2$BGC)
LHCUnits <- unique(X2_LHC$BGC)
FewUnits <- unique(X2_Few$BGC)

```

Large BGCs are subsampled using a cLHS of the 35-variable space for each BGC removing training points down to that specified in the rescaling. 
``` {r generate the cLHS, cache = TRUE}
#LHCUnits = c("CCHun_CA", "BAFAun")
# LHCUnits = c("BAFAun", "BGdh_OR", "CCHun_CA", "MGPmw_MT", "MGPdm")
#BGC <- c("BSJPap")
tic()
worker.init <- function(){
    Rcpp::sourceCpp("D:/GitHub/PEM_Methods_DevX/_functions/CppCLHS.cpp")
}

source("D:/GitHub/PEM_Methods_DevX/_functions/FastCLHS_R.R")
Rcpp::sourceCpp("D:/GitHub/PEM_Methods_DevX/_functions/CppCLHS.cpp")

cl <- makeCluster(detectCores()-2)
clusterCall(cl,fun = worker.init)
registerDoParallel(cl)

LHCtraining <- foreach(BGC = LHCUnits, .combine = rbind, .noexport = c("c_cor","obj_fn"), .packages = c("Rcpp", "clhs", "caret", "dplyr", "foreach"),.errorhandling="remove") %dopar% { #
  temp <- X2[(X2$BGC %in% BGC),]
  temp <- temp[,-c(2)]
  temp <- na.omit(temp)
  temp$xx <- ""
  temp <- droplevels(temp)
  Num <- X2_LHC$n[(X2_LHC$BGC %in% BGC)]
  nz <- nearZeroVar(temp, names=T) # remove near zero variance variables which cause cLHS to fail
   samples <- clhs_fast(temp[, -which (names(temp) %in% nz)], #CMD with all zeros precluded sampling of coastal units
                  size = Num,           # Test a range of sample sizes
                  iter = 100,        # Arbitrarily large number of iterations of optimization procedure. Default=10,000 but a larger number may be used
                  progress = FALSE, obj.limit = -Inf, eta = 1,
                  simple = FALSE)

  cLHS <- as.data.frame(samples$sampled_data)

  cLHS <-  cLHS %>% dplyr::select(ID1)#,BGC)
  cLHS$ID1 <- row.names(cLHS)###CHECK THIS
  cLHS_Points <- merge (cLHS, X2,  by = "ID1")
  cLHS_Points
}

toc()

LHCtraining <- droplevels(LHCtraining)
LHCtraining.list <- unique(LHCtraining$BGC)
X2_LHCmissed <- X2_LHC[!(X2_LHC$BGC %in% LHCtraining.list),] #catch any units where the cLHS failed
X2_LHCmissed <- droplevels(X2_LHCmissed)
X2_LHCmissed
countLHC <- LHCtraining %>% count(BGC) ###about 265
runLHC <- unique(LHCtraining$BGC)
LHC_missed <- LHCUnits[!LHCUnits %in% runLHC]
#LHC_missed <- droplevels(LHC_missed)
LHC2 <-as.data.frame(LHC_missed)
LHC_toadd <- X2[X2$BGC %in% LHC_missed,]
LHC_toadd <- droplevels(LHC_toadd)

```

 Under trained BGC units are upsampled using the SMOTE to add synthetic points up to the number specified in the resampling.
``` {r add SMOTE units to those BGCs that are undersampled, cache = TRUE}
X2_Smote <- X0temp
X2_Smote <- X2[(X2$BGC %in% X2_Few$BGC),]
X2_Smote <- X2_Smote %>% mutate_if(is.integer,as.numeric) #select(-Latitude, -Longitude, -Zone)
X2_Smote$BGC  <- as.factor(X2_Smote$BGC)
X2_Smote$BGCnum <- as.numeric(X2_Smote$BGC)
X2_Smote <- droplevels(X2_Smote)
X2.list <- unique(X2_Smote$BGC)
countSmote <- X2_Smote %>% count(BGC)
smote.num <- X2_Few %>% select(BGC, ratio)
smote.list <- as.character(smote.num$BGC)
#smote.list$BGC <- as.character(smote.list$BGC)
#df <- smote.list %>% mutate(name = paste0(BGC, " = ", ratio))
#BGC = "BGxh1"
# smote.list <- as.pairlist(df$name)
#X2_Smote2 <- SmoteClassif(BGC ~ ., X2_Smote, C.perc = list(BGxh1 = 120),  K = 5 )# creates full balanced data set
# 
#samples <- smotefamily::SMOTE(X2_Smote[,-c(1:2)], X2_Smote$BGCnum , #CMD with all zeros precluded sampling of coastal units
#                 K = 2, dup_size = 2) 
BGC="BGxh1"
# ratio
X2_Smote2 <- foreach(BGC = smote.list, .combine = rbind, .packages = c("smotefamily", "caret", "dplyr"),.errorhandling="remove") %do% { #
  temp <- X2_Smote[(X2_Smote$BGC %in% BGC),]
  temp <- temp[,-c(1)]
  temp <- na.omit(temp)
  #temp$xx <- ""
  temp <- droplevels(temp)
  Num <- as.numeric(smote.num$ratio[(smote.num$BGC %in% BGC)])
  #nz <- nearZeroVar(temp, names=T)# remove near zero variance variables which cause cLHS to fail
  #nz <- nz[nz != "BGCnum"]

     #smote.exs <- function(data,tgt,N,k)   
    samples <- smotefamily::SMOTE(temp[-1], temp$BGCnum, #CMD with all zeros precluded sampling of coastal units
                  K = 2, dup_size = Num)# perc.under = 100, k=5,learner = NULL)           # ratio
                         
# 
#  samples <- SMOTE(BGC ~ ., data = temp[, -which (names(temp) %in% nz)], #CMD with all zeros precluded sampling of coastal units
#                  perc.over = Num, perc.under = 100, k=5,learner = NULL)           # ratio
#                          
  smote <- samples$data
  smote$BGC <- BGC
  smote$ID1 <- row.names(smote)
  smote_points <- smote %>% dplyr::select(-BGCnum, -class)
   smote_points <- smote_points %>% dplyr::select(ID1, BGC, everything())
  smote_points
}
# X2_Mid <- countSubzone[countSubzone$n <= 500 & countSubzone$n >300,] # use orginal numbers for those >300
# X2_Mid <- droplevels(X2_Mid)
# count_mid <- X2_Mid  %>% count(BGC)
# X2_OK2 <- X2[(X2$BGC %in% X2_Mid$BGC),]
# X2_OK2 <- droplevels(X2_OK2)
# X2_Smote3 <- X2_Smote2[!(X2_Smote2$BGC %in% X2_Mid$BGC),]
# X2_Smote3 <- droplevels(X2_Smote3)
# Smote_Final <- rbind (X2_Smote3, X2_OK2)
count_LHC <- LHCtraining %>% count(BGC)
count_OK <- X2_OK2 %>% count(BGC)
count_smote <- X2_Smote2 %>% count(BGC)

X2_1 <- rbind (LHCtraining, X2_Smote2 )
X2_2 <- rbind (X2_OK2, X2_1)

# X2_OK3 <- X2[(X2$BGC %in% X2_OK$BGC),]
# X2_OK3 <- droplevels(X2_OK3)

# LHC_Final3 <- rbind (LHC_Final2, X2_OK3)
# LHC_Final3 <- droplevels(LHC_Final3)
plyr::count(X2_2, vars = "BGC")
X2_final <- droplevels(X2_2)
countX <- X2_final %>% count(BGC)
#write.csv(X2_final, "./cLHS/WNA_Training_259_VARS_new.csv", row.names = FALSE)
#X2_final <-  fread("./cLHS/WNA_Training_9_VARS.csv", data.table = FALSE)

```


# ```{r find and create interactions}
# X0 <- fread("./inputs/WNA_4k_HexPts_BGC_Normal_1961_1990MSY.csv")
# X0[X0 == -9999] <- NA
# X0 <- X0[complete.cases(X0),]
# X0 <- addVars(X0)
# 
# X0 <- X0[,c("BGC",vs8), with = F] 
# X0[,target := grepl("ICH",BGC)][,target := ifelse(target,1,0)]
# temp <- X0[,.(target,CMD,MSP)][,melt(.SD,id.vars = "target")][,variable := as.factor(variable)]
# ggplot(temp, aes(x = value, y = target, col = variable)) +
#   geom_point()
# 
# ##logistic regressions to test specific interactions
# mod1 <- glm(target ~ CMD + MSP, data = X0, family = "binomial")
# anova(mod1, test = "Chisq")
# mod2 <- glm(target ~ CMD + MSP + CMD:MSP, data = X0, family = "binomial")
# anova(mod2, test = "Chisq")
# 
# anova(mod1,mod2,test = "LRT")##test decrease in residual sums of squares
# 
# mod1 <- glm(target ~ CMD + DD5_sp, data = X0, family = "binomial")
# anova(mod1, test = "Chisq")
# mod2 <- glm(target ~ CMD + DD5_sp + CMD:DD5_sp, data = X0, family = "binomial")
# anova(mod2, test = "Chisq")
# 
# anova(mod1,mod2,test = "LRT")
# 
# mod1 <- glm(target ~ CMD + MSP + DD5_sp, data = X0, family = "binomial")
# anova(mod1, test = "Chisq")
# mod2 <- glm(target ~ CMD + MSP + DD5_sp + CMD:MSP + CMD:DD5_sp + MSP:DD5_sp + CMD:MSP:DD5_sp, data = X0, family = "binomial")
# anova(mod2, test = "Chisq")
# 
# anova(mod1,mod2,test = "LRT")##test decrease in residual sums of squares
# 
# ###now trying some stuff from the Kuhn book
# library(glinternet)
# xmat <- as.matrix(X0[,!c("BGC","target")])
# test <- glinternet(X = xmat, Y = X0$target, numLevels = rep(1,35), family = "binomial")

#```