---
title: "Predict Biogeoclimatic Units for BC for Current and Future Time Periods"
author: "William H MacKenzie"
date: "22/03/2020"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
require (smotefamily)
#require (UBL) ## UBL conflicts with another package if loaded after a package in the list below
require(tcltk)
require(data.table)
library(randtoolbox)
library(clhs)
library(foreign)
library(ggplot2)
library(raster)
library(rgdal)
library(RStoolbox)
library(maptools)
library(sp)
library(spatstat)
require(doParallel)
require(scales)
require(data.table)
require(plyr)
require (DMwR)
require (dplyr)
require(caret)
require (gstat)
require (tidyr)
require(ranger)
require(rticles)
require(expss)
require(purrr)
require(forcats)
require(sf)
require(StatMatch)
require(randomForest)
#require (gstat)
#install.packages("smotefamily", dependencies = TRUE)
#data.dir = "./inputs/"
#dir.create("cLHS")

addVars <- function(dat){ ##this function modifies everything inplace, so no need for return value
  dat[,`:=`(PPT_MJ = PPT05+PPT06,
            PPT_JAS = PPT07+PPT08+PPT09,
            PPT.dormant = PPT_at+PPT_wt)]
  dat[,`:=`(CMD.def = 500-PPT.dormant)]
  dat[CMD.def < 0, CMD.def := 0]
  dat[,`:=`(CMDMax = CMD07,
            CMD.total = CMD.def +CMD)]
  dat[,`:=`(CMD.grow = CMD05+CMD06+CMD07+CMD08+CMD09,
            DD5.grow = DD5_05+DD5_06+DD5_07+DD5_08+DD5_09,
            DDgood = DD5 - DD18,
            DDnew = (DD5_05+DD5_06+DD5_07+DD5_08)-(DD18_05+DD18_06+DD18_07+DD18_08),
            TmaxJuly = Tmax07)]
}
```

Points from a 8km hex grid of western north america are generated in R and submitted to ClimateNA to extract annual and seasonal variables for the historic normal period (1961-90) and an ensemble future climate scenario (rcp45 2040-2070). These data sets are combined. Several additional climate variables are generated including several monthly climate sums for precipitation and growing degree days. All other monthly variables are removed. A winter rechange moisture deficit was calculated and summed with the climatic moisture deficit to account for regions that begin the growing season in soil moisture deficit.

```{r prep tile data}
tileDir <- "./inputs/DND_future"
region <- "DND"
tiles <- list.files(tileDir)
dat <- foreach(fname = tiles, .combine = rbind) %do% {
  temp <- fread(file.path(tileDir,fname))
  temp
}
rm(temp)
dat[,`:=`(Latitude = NULL, Longitude = NULL, Elevation = NULL)]

library(RPostgreSQL)
drv <- dbDriver("PostgreSQL")
##connect to local postgres instance
con <- dbConnect(drv, user = "postgres", host = "localhost",
                 password = "Kiriliny41", port = 5432, dbname = "cciss_data") 
temp <- dbGetQuery(con,paste0("SELECT DISTINCT siteno, tileno FROM id_atts WHERE dist_code = '",region,"'"))

dat <- dat[ID1 %in% temp$siteno,]
dat[,c("GCM","Scenario","FuturePeriod") := tstrsplit(Year, "_", fixed = T)]
data.table::setcolorder(dat,c(1:3,251:253,4:250))
dat[,Year := NULL]
setnames(dat,old = c("ID1","ID2"), new = c("siteno","BGC"))
dat[,FuturePeriod := gsub(".gcm","",FuturePeriod)]
fwrite(dat, "DND_EnsembleDat.csv")
```

```{r read in Hex Grid Climate Data}
region <- "DND"
<<<<<<< HEAD
# bring in grid data with climate attributes
=======
timeperiod <- 2055
scn <- "rcp45"
load("./outputs/WNA_Subzone_NoMonth.Rdata")
model_vars <- as.data.frame(BGCmodel$variable.importance) %>% tibble::rownames_to_column()
covcount <- count(model_vars)

if(timeperiod %in% c(2025,2055,2085)){
  dat <- fread("./inputs/DND_EnsembleDat.csv")
  dat <- dat[FuturePeriod == timeperiod & Scenario == scn,]
  addVars(dat)
  X2 <- dat
}else{
  # bring in grid data with climate attributes
>>>>>>> eaa6e57640b896935361e6980a90bc7eda0288ad
X <- fread(paste0("./inputs/", region,"_400m_HexPts_Normal_1961_1990MSY.csv"), stringsAsFactors = FALSE, data.table = FALSE)#read in historic period
addVars(X)

###combine and average decadal data for the current normal period
Y1 <- fread(paste0("./inputs/", region, "_400m_HexPts_Decade_1991_2000MSY.csv"), stringsAsFactors = FALSE, data.table = FALSE)#read in future ensemble period
Y2 <- fread(paste0("./inputs/", region, "_400m_HexPts_Decade_2001_2010MSY.csv"), stringsAsFactors = FALSE, data.table = FALSE)#read in future ensemble period
Y3 <- fread(paste0("./inputs/", region, "_400m_HexPts_Decade_2011_2019MSY.csv"), stringsAsFactors = FALSE, data.table = FALSE)#read in future ensemble period
Y <-  rbind (Y1,Y2,Y3)
addVars(Y)
# 
<<<<<<< HEAD
load("./BGC_models/WNA_Subzone_35Var_rf.Rdata")
model_vars <- as.data.frame(BGCmodel$variable.importance) %>% tibble::rownames_to_column()
model_vars <- as.data.frame(BGCmodel$importance) %>% tibble::rownames_to_column()
=======

#BGCmodel <- BGCmodel2
model_vars <- as.data.frame(BGCmodel$variable.importance) %>% tibble::rownames_to_column()
#model_vars <- as.data.frame(BGCmodel2$importance) %>% tibble::rownames_to_column()
>>>>>>> eaa6e57640b896935361e6980a90bc7eda0288ad
covcount <- count(model_vars)
X2 <- X1 %>% dplyr::select(ID1, model_vars [,1])# %>% drop_na(BGC)
timeperiod = "1961-1990"
Y <- Y %>% dplyr::select(ID1, model_vars [,1])
Y1 <- data.table(Y)
Y2 <- Y1[,lapply(.SD,mean),
                     by = .(ID1),]
#--------unremark for 1991-2019 period
X2 <- Y2 ## use this line if wanting to run the modern climate change period
timeperiod = "1991-2019"
#-----------------------

}

#X1$BGC <- as.factor(X1$BGC)
#write.csv(model_vars, "./outputs/16Var_gini_WNABGCv11.csv", row.names = FALSE)
#modelvars <- read.csv("./outputs/Final27Var_WNABGCv11.csv")


```

####Predict BGC membership of hex grid

``` {r predict BGC of hex points }
<<<<<<< HEAD
###Predict for ranger model
grid.pred <- predict(BGCmodel, data = X2[,-c(1)])
BGC <- as.data.frame(grid.pred$predictions) %>% rename("BGC" = "grid.pred$predictions")
grid.pred <- predict(BGCmodel, data = X2[,-c(1)])
BGC <- as.data.frame(grid.pred)#%>% tibble::rownames_to_column()# %>% rename("BGC" = "grid.pred$predicted")
X1.pred <- cbind(X2, BGC) %>% select(ID1, BGC)
=======
###Predict
grid.pred <- predict(BGCmodel, data = X2)
temp <- as.data.table(grid.pred$predictions)
BGC <- cbind(X2$siteno,temp)
setnames(BGC, c("ID1","BGC"))
# X1.pred <- X2
# X1.pred$BGC <- predict(BGCmodel2, newdata = X1.pred)
# X1.pred <- X1.pred[,.(ID1,BGC)]
#X1.pred <- cbind(X2, BGC) %>% select(ID1, BGC)
X1.pred <- BGC
>>>>>>> eaa6e57640b896935361e6980a90bc7eda0288ad
X1.pred$BGC <-  fct_explicit_na(X1.pred$BGC , na_level = "(None)")
```

# Attribute hex grid with subzone/variant call
```{r link to hex polygon layer}
require(lwgeom)
##############link predicted Zones to Polygons and write shape file

##pull from db

q <- paste0("SELECT * FROM sf_grd WHERE dist_code = 'DND'")
hexdat <- st_read(con, query = q)
hexdat <- hexdat["siteno"]
st_crs(hexdat) <- 3005

# hexpoly <- st_read(dsn = paste0("./inputs/", region, "_bgc_hex400.gpkg"))#, layer = "USA_bgc_hex_800m")
# hexpoly$hex_id <- as.character(hexpoly$hex_id)
hexpoly <- as.data.table(hexdat) %>% st_as_sf()
hexZone <- X1.pred[hexpoly, on = c(ID1 = "siteno")]
hexZone <- st_as_sf(hexZone, crs = 3005) %>% st_cast() %>% select(BGC, geom)
hexZone <- st_zm(hexZone, drop=T, what='ZM') 
st_precision(hexZone) <- .5
hexZone <- as.data.table(hexZone)
hexComb <- hexZone[,.(geom = st_union(geom)), by = .(BGC)]
hexComb <- st_as_sf(hexComb) %>% st_cast()
st_write(hexComb, dsn = paste0("./outputs/", region, "_", "SubZoneMap_hex400_dissolved2.gpkg"), layer = paste0(region, "_", timeperiod, "_", covcount, "_ranger"), driver = "GPKG", delete_layer = TRUE)

#mapView(t2)
BGC_area <- t2 %>%
  mutate(Area = st_area(.)) %>% mutate (Area = Area/1000000) %>%
  mutate(ID = seq_along(BGC)) %>% dplyr::select(BGC, Area) %>% st_set_geometry(NULL)
write.csv(BGC_area, paste0("./outputs/", region, "_", timeperiod, "_", covcount, "_BGC_area_predicted.csv"))


```


```{r clean crumbs}
###now cleanup and remove crumbs
library(units)
t3 <- st_cast(t2, "MULTIPOLYGON") %>% st_cast("POLYGON")
t3 <- t3 %>%
  mutate(Area = st_area(.)) %>%
  mutate(ID = seq_along(BGC))
#unique(t3$Area)



size <- 300000
size <- set_units(size, "m^2")
t3$Area <- set_units(size, "m^2")
tSmall <- t3[t3$Area <= size,]
t3$BGC <- as.character(t3$BGC)

require(doParallel)
coreNum <- as.numeric(detectCores()-1)
coreNo <- makeCluster(coreNum)
registerDoParallel(coreNo, cores = coreNum)

###loop through each polygon < size, determine intersects, and assign to zone with most edge touching
###all the built in functions I found only dealt with holes in the middle of polygons
i = 1
new <- foreach(i = 1:length(tSmall$ID), .combine = rbind, .packages = c("foreach","sf")) %dopar% {
  ID <- tSmall$ID[i]
  nbrs <- st_intersects(tSmall[i,],t3)[[1]]
  nbrs <- nbrs[!nbrs %in% ID]
  if(length(nbrs) == 0){return(NULL)}
  lines <- st_intersection(t3[ID,],t3[nbrs,])
  lines <- st_cast(lines)
  l.len <- st_length(lines)
  names(l.len) <- lines$BGC.1
  zn <- names(l.len)[l.len == max(l.len)][1]
  newDat <- t3[ID,]
  newDat$BGC <- zn
  newDat
}

stopCluster(coreNo)
gc()
temp <- t3[!t3$ID %in% new$ID,]
t3 <- rbind(temp, new) %>%
  mutate(Zone = as.factor(BGC))

###now have to combine crumbs with existing large polygons
temp2 <- t3
st_precision(temp2) <- 0.5
t3 <- temp2 %>%
  group_by(BGC) %>%
  summarise(geometry = sf::st_union(geometry)) %>%
  ungroup()

#mapview(t2, zcol = "BGC")
t3 <- st_zm(t3, drop=T, what='ZM')
t3 <- t3 %>% st_buffer (0)
st_write(t3, dsn = paste0("./outputs/", region, "_SubZoneMap_1991_2019_eliminated.gpkg"), driver = "GPKG")

```


