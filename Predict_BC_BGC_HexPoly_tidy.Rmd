---
title: "Predict Biogeoclimatic Units for BC for Current and Future Time Periods"
author: "William H MacKenzie"
date: "22/03/2020"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
require (smotefamily)
#require (UBL) ## UBL conflicts with another package if loaded after a package in the list below

require(data.table)
library(foreign)
library(ggplot2)
library(raster)
library(rgdal)
library(RStoolbox)
library(maptools)
library(sp)
library(spatstat)
require(doParallel)
require(scales)
require(data.table)
require(plyr)
require (DMwR)
require (dplyr)
require(caret)
require (gstat)
require (tidyr)
require(ranger)
require(rticles)
require(expss)
require(purrr)
require(forcats)
require(sf)
require(StatMatch)
require(randomForest)
require(tidymodels)
#require (gstat)
#install.packages("smotefamily", dependencies = TRUE)
#data.dir = "./inputs/"
#dir.create("cLHS")

source("./_functions/AddVars.R")

```

Points from a 8km hex grid of western north america are generated in R and submitted to ClimateNA to extract annual and seasonal variables for the historic normal period (1961-90) and an ensemble future climate scenario (rcp45 2040-2070). These data sets are combined. Several additional climate variables are generated including several monthly climate sums for precipitation and growing degree days. All other monthly variables are removed. A winter rechange moisture deficit was calculated and summed with the climatic moisture deficit to account for regions that begin the growing season in soil moisture deficit.

```{r read in Hex Grid Climate Data}
region <- "DND"
# bring in grid data with climate attributes
#  X <- fread(paste0("./inputs/", region,"_400m_HexPts_Normal_1961_1990MSY.csv"), stringsAsFactors = FALSE, data.table = FALSE)#read in historic period
# hexpts <- X %>% select(ID1, ID2, Latitude, Longitude, Elevation)
# #fwrite(hexpts,paste0("./inputs/", region, "_400m_HexPts.csv"))
# X1 <- addVars(X)

###read in the current normal period 1991-2020
Y <- fread(paste0("./inputs/", region, "_400m_HexPts_Normal_1991_2020MSY.csv"))
Y <- addVars(Y)

#read in future ensemble period
 Z <- fread(paste0("./inputs/", region,"_400m_HexPts_5 GCMsMSY.csv"), stringsAsFactors = FALSE, data.table = FALSE)#read in historic period
 models <- as.data.frame(unique(Z$Year))
 model <-  c("13GCMs_ensemble_ssp245_2061-2080.gcm")
 Z1 <-  Z %>% filter(Year %in% model) %>% select(-Year)
 Z1 <-addVars(Z1)

### load  tidy model
 #load("./BGC_models/WNAv12_Zone_13_Var_tidy_30June21.RData")
 #  load("./BGC_models/WNAv12_Subzone_13_Var_tidy_30June21.RData")
 # BGCmodel.tidy <- BGC_rfzone
 # model_vars <- as.data.frame(BGCmodel.tidy$fit$fit$fit$variable.importance) %>% tibble::rownames_to_column()

### load ranger  model

load("./BGC_models/WNAv12_Subzone_13_Var_ranger_30June21.Rdata")
BGCmodel <- BGCmodel2
model_vars <- as.data.frame(BGCmodel$variable.importance) %>% tibble::rownames_to_column()

covcount <- nrow(model_vars)
# X2 <- X %>% dplyr::select(ID1, model_vars [,1])# %>% drop_na(BGC)
# timeperiod = "1961-1990"
idLocs <- Y[,.(ID1,Latitude,Longitude)] 
Y <- Y %>% dplyr::select(ID1, model_vars [,1])
Y1 <- data.table(Y)
Y2 <- Y1[,lapply(.SD,mean),
                     by = .(ID1),]
#--------unremark for 1991-2019 period
X2 <- Y2 ## use this line if wanting to run the modern climate change period
timeperiod = "1991-2020"

# XX <- X %>% dplyr::select(ID1, ID2)
# Ycheck <- left_join(XX, Y1) %>% dplyr::select(ID1, ID2, everything())
# # 
X2 <- Z1 %>% dplyr::select(ID1, model_vars [,1])# %>% drop_na(BGC)
timeperiod = "2070"
-----------------------

#X1$BGC <- as.factor(X1$BGC)
#write.csv(model_vars, "./outputs/16Var_gini_WNABGCv11.csv", row.names = FALSE)
#modelvars <- read.csv("./outputs/Final27Var_WNABGCv11.csv")


```

####Predict BGC membership of hex grid

``` {r predict BGC of hex points }
# BGC_recipe <- recipe(BGC ~ .,  data = XAll) %>%
#   update_role(ID1, new_role = "ID") %>% 
#   step_center(all_predictors()) %>%
#   step_scale(all_predictors()) %>%  prep(training = XAll, retain = TRUE)
# 
# X2.juice <- bake(BGC_recipe, X2)



###Predict for tidy model
# X2.id <- as.data.frame(X2$ID1)
# 
# grid.pred <- predict(BGCmodel.tidy, new_data = X2)
# grid.pred <- cbind(X2.id, grid.pred $.pred_class)
# X1.pred <- as.data.frame(grid.pred) %>% dplyr::rename(ID1 = 1, BGC = 2)
#  X1.pred$BGC <-  fct_explicit_na(X1.pred$BGC , na_level = "(None)")
#  X1.pred$ID1 <- as.character(X1.pred$ID1)

###Predict for ranger model

 grid.pred <- predict(BGCmodel, data = X2[,-c(1)])
 BGC <- as.data.frame(grid.pred$predictions)%>% tibble::rownames_to_column() %>% dplyr::rename("BGC" = "grid.pred$predictions")
X1.pred <- cbind(X2, BGC) %>% select(ID1, BGC)
X1.pred$BGC <-  fct_explicit_na(X1.pred$BGC , na_level = "(None)")
X1.pred$ID1 <- as.character(X1.pred$ID1)

```

```{r join bec}
bgc <- st_read("~/CommonTables/BC_BGCv12.gpkg")
idLocs2 <- st_as_sf(idLocs, coords = c("Longitude","Latitude"), crs = 4326, agr = "constant")
idLocs2 <- st_transform(idLocs2, 3005)
bgc <- st_zm(bgc)
bgc <- st_cast(bgc,"MULTIPOLYGON")
bgcID <- st_join(idLocs2,bgc,left = T)
bgcID <- as.data.table(st_drop_geometry(bgcID))
BGC <- as.data.table(BGC)
setnames(BGC, c("ID1","BGC.pred"))
BGC[,ID1 := as.integer(ID1)]
BGC[bgcID, BGC := i.BGC, on = "ID1"]
BGC[,Change := paste0(BGC,"-",BGC.pred)]
allChanges <- table(BGC$Change)
sort(allChanges)

##lets try SBSdk -> IDFdk1
```

# Attribute hex grid with subzone/variant call
```{r link to hex polygon layer}
require(lwgeom)
##############link predicted Zones to Polygons and write shape file

hexpoly <- st_read(dsn = paste0("./hexpolys/", region, "_bgc_hex400.gpkg"))#, layer = "USA_bgc_hex_800m")
hexpoly$hex_id <- as.character(hexpoly$hex_id)
hexZone <- left_join(hexpoly, X1.pred, by = c("hex_id" = "ID1"))%>% st_transform(3005) %>% st_cast()
temp <- hexZone %>% select(BGC, geom)
temp2 <- st_zm(temp, drop=T, what='ZM') 
##unremark for undissolved layer
#st_write(temp2, dsn = paste0("./outputs/", region, "_", "SubZoneMap_hex400_undissolved.gpkg"), driver = "GPKG", delete_dsn = TRUE)

## unremark for Dissolved
##hexZone <- st_read(dsn = "./outputs/WA_bgc_hex8000_ungrouped.gpkg")#, layer = "USA_bgc_hex_800m") ## need to read it back in to ensure all type Polygon is consistent
temp3 <- hexZone
temp3$BGC <- droplevels(temp3$BGC)
temp3 <-  st_as_sf(temp3)#
st_precision(temp3) <- .5
temp3$BGC <- forcats::fct_explicit_na(temp3$BGC,na_level = "(None)")
temp3 <- temp3[,c("BGC","geom")]
t2 <- aggregate(temp3[,-1], by = list(temp3$BGC), do_union = T, FUN = mean) %>% dplyr::rename(BGC = Group.1)
t2 <- st_zm(t2, drop=T, what='ZM') %>% st_transform(3005) %>% st_cast()

t2 <- t2 %>% st_buffer(0) ## randomly fixes geometry problems
#mapView(t2)
BGC_area <- t2 %>%
  mutate(Area = st_area(.)) %>% mutate (Area = Area/1000000) %>%
  mutate(ID = seq_along(BGC)) %>% dplyr::select(BGC, Area) %>% st_set_geometry(NULL)
#write.csv(BGC_area, paste0("./outputs/", region, "_", timeperiod, "_", covcount, "_BGC_area_predicted_mahbgcreduced.csv"))
st_write(t2, dsn = paste0("./outputs/", covcount, "_", "Subzone_Map_hex400_dissolved_reduced30June_3.gpkg"), layer = paste0(region, "_", timeperiod, "_", region ,"_", covcount, "_vars_ranger"), driver = "GPKG", delete_layer = TRUE)
                                                                                                       

```


```{r clean crumbs}
###now cleanup and remove crumbs
library(units)
t3 <- st_cast(t2, "MULTIPOLYGON") %>% st_cast("POLYGON")
t3 <- t3 %>%
  mutate(Area = st_area(.)) %>%
  mutate(ID = seq_along(BGC))
#unique(t3$Area)



size <- 300000
size <- set_units(size, "m^2")
t3$Area <- set_units(size, "m^2")
tSmall <- t3[t3$Area <= size,]
t3$BGC <- as.character(t3$BGC)

require(doParallel)
coreNum <- as.numeric(detectCores()-1)
coreNo <- makeCluster(coreNum)
registerDoParallel(coreNo, cores = coreNum)

###loop through each polygon < size, determine intersects, and assign to zone with most edge touching
###all the built in functions I found only dealt with holes in the middle of polygons
i = 1
new <- foreach(i = 1:length(tSmall$ID), .combine = rbind, .packages = c("foreach","sf")) %dopar% {
  ID <- tSmall$ID[i]
  nbrs <- st_intersects(tSmall[i,],t3)[[1]]
  nbrs <- nbrs[!nbrs %in% ID]
  if(length(nbrs) == 0){return(NULL)}
  lines <- st_intersection(t3[ID,],t3[nbrs,])
  lines <- st_cast(lines)
  l.len <- st_length(lines)
  names(l.len) <- lines$BGC.1
  zn <- names(l.len)[l.len == max(l.len)][1]
  newDat <- t3[ID,]
  newDat$BGC <- zn
  newDat
}

stopCluster(coreNo)
gc()
temp <- t3[!t3$ID %in% new$ID,]
t3 <- rbind(temp, new) %>%
  mutate(Zone = as.factor(BGC))

###now have to combine crumbs with existing large polygons
temp2 <- t3
st_precision(temp2) <- 0.5
t3 <- temp2 %>%
  group_by(BGC) %>%
  summarise(geometry = sf::st_union(geometry)) %>%
  ungroup()

#mapview(t2, zcol = "BGC")
t3 <- st_zm(t3, drop=T, what='ZM')
t3 <- t3 %>% st_buffer (0)
st_write(t3, dsn = paste0("./outputs/", region, "_SubZoneMap_1991_2019_eliminated.gpkg"), driver = "GPKG")

```


