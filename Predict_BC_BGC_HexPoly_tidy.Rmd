---
title: "Predict Biogeoclimatic Units for BC for Current and Future Time Periods"
author: "William H MacKenzie"
date: "22/03/2020"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
require (smotefamily)
#require (UBL) ## UBL conflicts with another package if loaded after a package in the list below
require(tcltk)
require(data.table)
library(randtoolbox)
library(clhs)
library(foreign)
library(ggplot2)
library(raster)
library(rgdal)
library(RStoolbox)
library(maptools)
library(sp)
library(spatstat)
require(doParallel)
require(scales)
require(data.table)
require(plyr)
require (DMwR)
require (dplyr)
require(caret)
require (gstat)
require (tidyr)
require(ranger)
require(rticles)
require(expss)
require(purrr)
require(forcats)
require(sf)
require(StatMatch)
require(randomForest)
require(tidymodels)
#require (gstat)
#install.packages("smotefamily", dependencies = TRUE)
#data.dir = "./inputs/"
#dir.create("cLHS")

addVars <- function(dat){
  dat$PPT_MJ <- dat$PPT05 + dat$PPT06  # MaY/June precip
  dat$PPT_JAS <- dat$PPT07 + dat$PPT08 + dat$PPT09  # July/Aug/Sept precip
  dat$PPT.dormant <- dat$PPT_at + dat$PPT_wt  # for calculating spring deficit
  dat$CMD.def <- 500 - (dat$PPT.dormant)  # start of growing season deficit original value was 400 but 500 seems better
  dat$CMD.def[dat$CMD.def < 0] <- 0  #negative values set to zero = no deficit
  dat$CMDMax <- dat$CMD07
  dat$CMD.total <- dat$CMD.def + dat$CMD
  dat$CMD.grow <- dat$CMD05 + dat$CMD06 +dat$CMD07 +dat$CMD08 +dat$CMD09
  dat$DD5.grow <- dat$DD5_05 + dat$DD5_06 + dat$DD5_07 + dat$DD5_08 + dat$DD5_09
  dat$CMDMax <- dat$CMD07 # add in so not removed below
  dat$DDgood <- dat$DD5 - dat$DD18
  dat$DDnew <- (dat$DD5_05 + dat$DD5_06 +dat$DD5_07  + dat$DD5_08)  - (dat$DD18_05 + dat$DD18_06 +dat$DD18_07 +dat$DD18_08)
  dat$TmaxJuly <- dat$Tmax07
  dat$Boreal <- ifelse(dat$EMT <=-40, 1, 0)
  # dat$Snow_wt <- ifelse(dat$Tave_wt >0, 0, dat$PAS_wt)
  # dat$Snow_at <- ifelse(dat$Tave_at >0, 0, dat$PAS_at)
  # dat$Snow_sp <- ifelse(dat$Tave_sp >0, 0, dat$PAS_sp)
  # dat$Snow <- dat$Snow_wt + dat$Snow_at + dat$Snow_sp
  #dat$CCI <- (1.7*(dat$MWMT - dat$MCMT) / sin(dat$Latitude +10))-14
   dat$CCI <- (1.7* (dat$MWMT - dat$MCMT/sin(dat$Latitude +10)))-14
  
  dat$KOI <- (100*(dat$Tave10 - dat$Tave04))/ (dat$MWMT - dat$MCMT)
  #   
  #   (dat$PAS10 + dat$PAS11  + dat$PAS12 + dat$PAS01 + dat$PAS02 + dat$PAS03 + dat$PAS04 ) - ((dat$DD5_11 + dat$DD5_wt  + dat$DD5_03) * 50)
  # dat$Snow <- ifelse (dat$Snow <0, 0, dat$Snow)
  return(dat)
}
```

Points from a 8km hex grid of western north america are generated in R and submitted to ClimateNA to extract annual and seasonal variables for the historic normal period (1961-90) and an ensemble future climate scenario (rcp45 2040-2070). These data sets are combined. Several additional climate variables are generated including several monthly climate sums for precipitation and growing degree days. All other monthly variables are removed. A winter rechange moisture deficit was calculated and summed with the climatic moisture deficit to account for regions that begin the growing season in soil moisture deficit.

```{r read in Hex Grid Climate Data}
region <- "DND"
# bring in grid data with climate attributes
X <- fread(paste0("./inputs/", region,"_400m_HexPts_Normal_1961_1990MSY.csv"), stringsAsFactors = FALSE, data.table = FALSE)#read in historic period
X1 <- addVars(X)

###combine and average decadal data for the current normal period
Y1 <- fread(paste0("./inputs/", region, "_400m_HexPts_Decade_1991_2000MSY.csv"), stringsAsFactors = FALSE, data.table = FALSE)#read in future ensemble period
Y2 <- fread(paste0("./inputs/", region, "_400m_HexPts_Decade_2001_2010MSY.csv"), stringsAsFactors = FALSE, data.table = FALSE)#read in future ensemble period
Y3 <- fread(paste0("./inputs/", region, "_400m_HexPts_Decade_2011_2019MSY.csv"), stringsAsFactors = FALSE, data.table = FALSE)#read in future ensemble period
Y <-  rbind (Y1,Y2,Y3)
Y <- addVars(Y)

# Z <- fread(paste0("./inputs/", region,"_400m_HexPts_6 GCMsMSY.csv"), stringsAsFactors = FALSE, data.table = FALSE)#read in historic period
# models <- as.data.frame(unique(Z$Year))
# model <-  c("15GCM-Ensemble_rcp45_2025.gcm")
# Z1 <- addVars(Z) %>% filter(Year %in% model) %>% select(-Year)


## load tidy model
# load("./BGC_models/WNAv12_Subzone_35_Var_ranger2.Rdata")
# BGCmodel.var <- pull_workflow_fit(BGCmodel)$fit
# model_vars <- BGCmodel.var$importance
# model_vars <- as.data.frame(model_vars) %>% tibble::rownames_to_column()

## load ranger model
load("./BGC_models/WNAv12_Subzone_24_Var_ranger_MDreduced3_DropSomeDistricts_alwayssplit.Rdata")
model_vars <- as.data.frame(BGCmodel2$variable.importance) %>% tibble::rownames_to_column()
BGCmodel <- BGCmodel2

covcount <- count(model_vars)
X2 <- X1 %>% dplyr::select(ID1, model_vars [,1])# %>% drop_na(BGC)
timeperiod = "1961-1990"
Y <- Y %>% dplyr::select(ID1, model_vars [,1])
Y1 <- data.table(Y)
Y2 <- Y1[,lapply(.SD,mean),
                     by = .(ID1),]
#--------unremark for 1991-2019 period
X2 <- Y2 ## use this line if wanting to run the modern climate change period
timeperiod = "1991-2019"
# 
# X2 <- Z1 %>% dplyr::select(ID1, model_vars [,1])# %>% drop_na(BGC)
# timeperiod = "2025"
#-----------------------

#X1$BGC <- as.factor(X1$BGC)
#write.csv(model_vars, "./outputs/16Var_gini_WNABGCv11.csv", row.names = FALSE)
#modelvars <- read.csv("./outputs/Final27Var_WNABGCv11.csv")


```

####Predict BGC membership of hex grid

``` {r predict BGC of hex points }
###Predict for tidy model
# grid.pred <- predict(BGCmodel, new_data = X2[,-c(1)])
# BGC <- as.data.frame(grid.pred$.pred_class) %>% rename("BGC" = "grid.pred$.pred_class")

###Predict for ranger model

 grid.pred <- predict(BGCmodel, data = X2[,-c(1)])
 BGC <- as.data.frame(grid.pred$predictions)%>% tibble::rownames_to_column() %>% rename("BGC" = "grid.pred$predictions")
X1.pred <- cbind(X2, BGC) %>% select(ID1, BGC)
X1.pred$BGC <-  fct_explicit_na(X1.pred$BGC , na_level = "(None)")
X1.pred$ID1 <- as.character(X1.pred$ID1)

```

# Attribute hex grid with subzone/variant call
```{r link to hex polygon layer}
require(lwgeom)
##############link predicted Zones to Polygons and write shape file

hexpoly <- st_read(dsn = paste0("./hexpolys/", region, "_bgc_hex400.gpkg"))#, layer = "USA_bgc_hex_800m")
hexpoly$hex_id <- as.character(hexpoly$hex_id)
hexZone <- left_join(hexpoly, X1.pred, by = c("hex_id" = "ID1"))%>% st_transform(3005) %>% st_cast()
temp <- hexZone %>% select(BGC, geom)
temp2 <- st_zm(temp, drop=T, what='ZM') 
##unremark for undissolved layer
#st_write(temp2, dsn = paste0("./outputs/", region, "_", "SubZoneMap_hex400_undissolved.gpkg"), driver = "GPKG", delete_dsn = TRUE)

## unremark for Dissolved
##hexZone <- st_read(dsn = "./outputs/WA_bgc_hex8000_ungrouped.gpkg")#, layer = "USA_bgc_hex_800m") ## need to read it back in to ensure all type Polygon is consistent
temp3 <- hexZone
temp3$BGC <- droplevels(temp3$BGC)
temp3 <-  st_as_sf(temp3)#
st_precision(temp3) <- .5
temp3$BGC <- forcats::fct_explicit_na(temp3$BGC,na_level = "(None)")
temp3 <- temp3[,c("BGC","geom")]
t2 <- aggregate(temp3[,-1], by = list(temp3$BGC), do_union = T, FUN = mean) %>% rename(BGC = Group.1)
t2 <- st_zm(t2, drop=T, what='ZM') %>% st_transform(3005) %>% st_cast()

t2 <- t2 %>% st_buffer(0)
#mapView(t2)
BGC_area <- t2 %>%
  mutate(Area = st_area(.)) %>% mutate (Area = Area/1000000) %>%
  mutate(ID = seq_along(BGC)) %>% dplyr::select(BGC, Area) %>% st_set_geometry(NULL)
write.csv(BGC_area, paste0("./outputs/", region, "_", timeperiod, "_", covcount, "_BGC_area_predicted_MDreduced.csv"))
st_write(t2, dsn = paste0("./outputs/", covcount, "_", "Subzone_Map_hex400_dissolved.gpkg"), layer = paste0(region, "_", timeperiod, "_", region , "_vars_ranger_MD_District5"), driver = "GPKG", delete_layer = TRUE)
                                                                                                       

```


```{r clean crumbs}
###now cleanup and remove crumbs
library(units)
t3 <- st_cast(t2, "MULTIPOLYGON") %>% st_cast("POLYGON")
t3 <- t3 %>%
  mutate(Area = st_area(.)) %>%
  mutate(ID = seq_along(BGC))
#unique(t3$Area)



size <- 300000
size <- set_units(size, "m^2")
t3$Area <- set_units(size, "m^2")
tSmall <- t3[t3$Area <= size,]
t3$BGC <- as.character(t3$BGC)

require(doParallel)
coreNum <- as.numeric(detectCores()-1)
coreNo <- makeCluster(coreNum)
registerDoParallel(coreNo, cores = coreNum)

###loop through each polygon < size, determine intersects, and assign to zone with most edge touching
###all the built in functions I found only dealt with holes in the middle of polygons
i = 1
new <- foreach(i = 1:length(tSmall$ID), .combine = rbind, .packages = c("foreach","sf")) %dopar% {
  ID <- tSmall$ID[i]
  nbrs <- st_intersects(tSmall[i,],t3)[[1]]
  nbrs <- nbrs[!nbrs %in% ID]
  if(length(nbrs) == 0){return(NULL)}
  lines <- st_intersection(t3[ID,],t3[nbrs,])
  lines <- st_cast(lines)
  l.len <- st_length(lines)
  names(l.len) <- lines$BGC.1
  zn <- names(l.len)[l.len == max(l.len)][1]
  newDat <- t3[ID,]
  newDat$BGC <- zn
  newDat
}

stopCluster(coreNo)
gc()
temp <- t3[!t3$ID %in% new$ID,]
t3 <- rbind(temp, new) %>%
  mutate(Zone = as.factor(BGC))

###now have to combine crumbs with existing large polygons
temp2 <- t3
st_precision(temp2) <- 0.5
t3 <- temp2 %>%
  group_by(BGC) %>%
  summarise(geometry = sf::st_union(geometry)) %>%
  ungroup()

#mapview(t2, zcol = "BGC")
t3 <- st_zm(t3, drop=T, what='ZM')
t3 <- t3 %>% st_buffer (0)
st_write(t3, dsn = paste0("./outputs/", region, "_SubZoneMap_1991_2019_eliminated.gpkg"), driver = "GPKG")

```


